{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# v 0.3.1\n",
    "import regex as re,string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords \n",
    "stop_words = set(stopwords.words('english'))\n",
    "from nltk.stem import SnowballStemmer\n",
    "stemmer= SnowballStemmer('english')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "import os.path\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#removes all the blank line from the text file\n",
    "#returns list\n",
    "def clear_blank_lines(file):\n",
    "    return document(file).clear_blank_lines().data\n",
    "\n",
    "# it removes \".\\n\" from every element by default\n",
    "# can be used to strip by second argument\n",
    "def strip_all(file,x='.\\n'):\n",
    "    return document(file).strip_all(x).data\n",
    "\n",
    "\n",
    "# converts each character to lowercase\n",
    "def lower_all(file):\n",
    "    return document(file).lower_all().data\n",
    "\n",
    "# removes numbers detected anywhere in the data\n",
    "def remove_numbers(file):\n",
    "    return document(file).remove_numbers().data\n",
    "\n",
    "# removes punctuations detected anywhere in the data\n",
    "def remove_symbols(file):\n",
    "    return document(file).remove_symbols().data\n",
    "\n",
    "# it will remove stop words and return a list of list of words  \n",
    "def remove_stpwrds(file,op='sents'):\n",
    "    return document(file).remove_stpwrds().data\n",
    "\n",
    "#for tokenization\n",
    "def token_it(file):\n",
    "    return document(file).token_it()\n",
    "\n",
    "# reduces each word to its stem work like, dogs to dog\n",
    "def stemming(file):\n",
    "    return document(file).stemming().data\n",
    "    \n",
    "# gets the root word for each word\n",
    "def lemming(file):\n",
    "    return document(file).lemming().data\n",
    "    \n",
    "\n",
    "def main_cleaner(file,op = 'sents'):\n",
    "    return document(file).main_cleaner().data\n",
    "\n",
    "def formating(block):\n",
    "    return [' '.join(each) for each in block]\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class document:\n",
    "    file_name = ''\n",
    "    data = []\n",
    "    words = []\n",
    "    tokens = []\n",
    "    count_sentences = 0\n",
    "    count_words = 0\n",
    "    each_word_count = {}\n",
    "    \n",
    "    def __init__(self,fname=''):\n",
    "        \n",
    "        if fname:            \n",
    "            self.data,fame =  self._reader(fname)\n",
    "            self.words = [each.split(' ') for each in self.clear_blank_lines().lower_all().strip_all().remove_numbers().remove_symbols()]\n",
    "            self.count_words = len([word for sent in self.words for word in sent])\n",
    "            self.count_sentences = sum(each.count('.') for each in self.data)\n",
    "            temp = self._flatlist(self.words)\n",
    "            self.each_word_count = {x:temp.count(x) for x in temp}\n",
    "            self.file_name = fame.split('\\\\')[-1]\n",
    "        else:\n",
    "            pass  \n",
    "        \n",
    "    def _flatlist(self,lis):\n",
    "        return [word for sent in lis for word in sent]\n",
    "        \n",
    "    def __repr__(self):\n",
    "         return '\\n'.join(map(str, self.data))\n",
    "\n",
    "    def __str__(self):\n",
    "        return '\\n'.join(map(str, self.data))\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for i in self.data: yield i\n",
    "    \n",
    "    def copy(self):\n",
    "        temp = copy.deepcopy(self)       \n",
    "        return temp\n",
    "\n",
    "    # checks whether it has file path as argument\n",
    "    def _file_or_not(self,arg):\n",
    "        if os.path.isfile(arg):\n",
    "            return True\n",
    "        else:\n",
    "            return False,\"only supports .txt for now\"\n",
    "\n",
    "    # this reader is flexible enough to process file or will return the data if list is being passed to the function.\n",
    "    def _reader(self,file):\n",
    "        if type(file)==str and self._file_or_not(file)==True:\n",
    "            with open(file,'r') as f:\n",
    "                return f.readlines(),file\n",
    "\n",
    "        elif type(file)==str:\n",
    "            return file.split('\\n'),''\n",
    "\n",
    "        else: return file,''\n",
    "\n",
    "\n",
    "    \n",
    "    #removes all the blank line from the text file\n",
    "    #returns list\n",
    "    def clear_blank_lines(self,inplace=False):\n",
    "        if not inplace: self = self.copy()\n",
    "        self.data =  list(filter(str.strip,[each.rstrip() for each in self.data]))\n",
    "        return self\n",
    "\n",
    "    \n",
    "    # it removes \".\\n\" from every element by default\n",
    "    # can be used to strip by second argument\n",
    "    def strip_all(self,x='.\\n',inplace=False):\n",
    "        if not inplace: self = self.copy()\n",
    "        self.data = [re.sub(r'\\s+',' ',each.strip(x)) for each in self.data]\n",
    "        return self\n",
    "\n",
    "\n",
    "    # converts each character to lowercase\n",
    "    def lower_all(self,inplace=False):\n",
    "        if not inplace: self = self.copy()\n",
    "        self.data = [each.lower() for each in self.data]\n",
    "        return self\n",
    "\n",
    "    # removes numbers detected anywhere in the data\n",
    "    def remove_numbers(self,inplace=False):\n",
    "        if not inplace: self = self.copy()\n",
    "        self.data = [re.sub(r'[0-9]+', '',(each)) for each in self.data]\n",
    "        return self\n",
    "\n",
    "    # removes punctuations detected anywhere in the data\n",
    "    def remove_symbols(self,inplace=False):\n",
    "        if not inplace: self = self.copy()\n",
    "        self.data = [re.sub(r'[^\\w\\s]','',each) for each in self.data]\n",
    "        return self.strip_all()\n",
    "\n",
    "    \n",
    "    # it will remove stop words and return a list of list of words  \n",
    "    def remove_stpwrds(self,inplace=False):\n",
    "        if not inplace: self = self.copy()\n",
    "        self.words = [[w for w in each.split() if not w in stop_words] for each in self.data] \n",
    "    \n",
    "        self.data = formating(self.words)        \n",
    "        return self\n",
    "        \n",
    "\n",
    "    #for tokenization this function can't be use as object \n",
    "    def token_it(self):\n",
    "        self.tokens = [word_tokenize(each) for each in self.data]\n",
    "        return self.tokens\n",
    "\n",
    "    # reduces each word to its stem work like, dogs to dog\n",
    "    def stemming(self,inplace=False):\n",
    "        if not inplace: self = self.copy()\n",
    "        self.data = formating([[stemmer.stem(word) for word in each.split()] for each in self.data])\n",
    "        return self\n",
    "\n",
    "    # gets the root word for each word\n",
    "    def lemming(self,inplace=False):\n",
    "        if not inplace: self = self.copy()\n",
    "        self.data = formating([[lemmatizer.lemmatize(word) for word in each.split()] for each in self.data])\n",
    "        return self\n",
    "\n",
    "\n",
    "    def main_cleaner(self,op = 'sents',inplace=False):\n",
    "        if not inplace: self = self.copy()\n",
    "            \n",
    "        # this is the basic cleaning which operates with each line\n",
    "        part1 = self.clear_blank_lines().strip_all().lower_all().remove_numbers().remove_symbols()\n",
    "\n",
    "        # this is the advanced cleaning which operates with each word\n",
    "        part2 = part1.lemming().remove_stpwrds()\n",
    "\n",
    "        if op== 'sents':\n",
    "            return part2\n",
    "\n",
    "        if op== 'words':\n",
    "            return [word for sent in part2.data for word in sent.split()]        \n",
    "\n",
    "        if op not in ('sents','words'):\n",
    "            return \"value of option is not valid, try 'sents' or 'words' instead\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1391"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document('file.txt').count_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = document('file.txt').each_word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'': 7,\n",
       " '_': 1,\n",
       " 'a': 25,\n",
       " 'abbreviations': 1,\n",
       " 'about': 2,\n",
       " 'above': 1,\n",
       " 'accent': 1,\n",
       " 'acquisition': 1,\n",
       " 'adjectives': 2,\n",
       " 'adt': 3,\n",
       " 'after': 2,\n",
       " 'aggressive': 1,\n",
       " 'aim': 1,\n",
       " 'aims': 2,\n",
       " 'algorithm': 4,\n",
       " 'algorithms': 2,\n",
       " 'all': 3,\n",
       " 'allows': 1,\n",
       " 'also': 5,\n",
       " 'among': 1,\n",
       " 'an': 6,\n",
       " 'analyses': 1,\n",
       " 'anaphora': 1,\n",
       " 'and': 33,\n",
       " 'andcc': 1,\n",
       " 'andrew': 2,\n",
       " 'any': 1,\n",
       " 'apache': 7,\n",
       " 'apartment': 2,\n",
       " 'apartmentnn': 1,\n",
       " 'apple': 1,\n",
       " 'applennp': 1,\n",
       " 'architecture': 4,\n",
       " 'are': 18,\n",
       " 'article': 2,\n",
       " 'as': 5,\n",
       " 'assign': 1,\n",
       " 'b': 2,\n",
       " 'balls': 4,\n",
       " 'base': 3,\n",
       " 'based': 1,\n",
       " 'bases': 1,\n",
       " 'basic': 1,\n",
       " 'be': 9,\n",
       " 'below': 1,\n",
       " 'biggest': 2,\n",
       " 'bill': 1,\n",
       " 'billnnp': 1,\n",
       " 'bing_key': 1,\n",
       " 'black': 2,\n",
       " 'blackjj': 1,\n",
       " 'blue': 2,\n",
       " 'booksbook': 1,\n",
       " 'boston': 1,\n",
       " 'bostonnnp': 1,\n",
       " 'bought': 2,\n",
       " 'boughtvbn': 1,\n",
       " 'box': 4,\n",
       " 'brazil': 2,\n",
       " 'break': 1,\n",
       " 'building': 2,\n",
       " 'buy': 1,\n",
       " 'by': 3,\n",
       " 'calais': 1,\n",
       " 'called': 1,\n",
       " 'can': 7,\n",
       " 'canonicalization': 1,\n",
       " 'car': 1,\n",
       " 'carry': 1,\n",
       " 'case': 1,\n",
       " 'cases': 1,\n",
       " 'categories': 1,\n",
       " 'chance': 1,\n",
       " 'chazz': 1,\n",
       " 'china': 2,\n",
       " 'chop': 1,\n",
       " 'chosen': 1,\n",
       " 'chunker': 1,\n",
       " 'chunking': 6,\n",
       " 'cities': 1,\n",
       " 'city': 1,\n",
       " 'classify': 1,\n",
       " 'code': 14,\n",
       " 'collocation': 5,\n",
       " 'collocationextractor': 1,\n",
       " 'collocationextractorwith_collocation_pipelinet': 1,\n",
       " 'collocations': 1,\n",
       " 'combinations': 1,\n",
       " 'common': 3,\n",
       " 'comparative': 1,\n",
       " 'complicated': 1,\n",
       " 'conclusion': 1,\n",
       " 'conference': 1,\n",
       " 'conferencenn': 1,\n",
       " 'connected': 1,\n",
       " 'considered': 1,\n",
       " 'considering': 1,\n",
       " 'constituent': 1,\n",
       " 'containing': 1,\n",
       " 'contains': 4,\n",
       " 'context': 1,\n",
       " 'convert': 2,\n",
       " 'converting': 2,\n",
       " 'core': 3,\n",
       " 'coreference': 7,\n",
       " 'corenlp': 4,\n",
       " 'correct': 1,\n",
       " 'countries': 2,\n",
       " 'created': 1,\n",
       " 'data': 2,\n",
       " 'definition': 1,\n",
       " 'describe': 1,\n",
       " 'described': 6,\n",
       " 'detail': 1,\n",
       " 'determine': 1,\n",
       " 'diacritics': 1,\n",
       " 'discrete': 1,\n",
       " 'discuss': 1,\n",
       " 'discussed': 1,\n",
       " 'dkpro': 3,\n",
       " 'do': 2,\n",
       " 'does': 1,\n",
       " 'done': 3,\n",
       " 'draw': 2,\n",
       " 'dtjjnn': 1,\n",
       " 'duel': 1,\n",
       " 'each': 2,\n",
       " 'easily': 2,\n",
       " 'eg': 2,\n",
       " 'emily': 1,\n",
       " 'emilys': 1,\n",
       " 'employment': 1,\n",
       " 'ending': 1,\n",
       " 'endings': 1,\n",
       " 'engineering': 4,\n",
       " 'english_stop_words': 1,\n",
       " 'entities': 2,\n",
       " 'entity': 2,\n",
       " 'etc': 3,\n",
       " 'example': 20,\n",
       " 'examples': 4,\n",
       " 'examplet': 1,\n",
       " 'expanding': 1,\n",
       " 'expected': 1,\n",
       " 'expressions': 2,\n",
       " 'extract': 1,\n",
       " 'extraction': 8,\n",
       " 'extractor': 1,\n",
       " 'false': 1,\n",
       " 'find': 1,\n",
       " 'finds': 1,\n",
       " 'first': 1,\n",
       " 'following': 1,\n",
       " 'for': 22,\n",
       " 'forin': 3,\n",
       " 'form': 2,\n",
       " 'format': 1,\n",
       " 'forms': 2,\n",
       " 'found': 2,\n",
       " 'free': 2,\n",
       " 'freeling': 3,\n",
       " 'from': 17,\n",
       " 'function': 1,\n",
       " 'further': 1,\n",
       " 'gate': 3,\n",
       " 'gateannie': 1,\n",
       " 'general': 4,\n",
       " 'generation': 1,\n",
       " 'gensim': 1,\n",
       " 'get': 2,\n",
       " 'given': 2,\n",
       " 'gpe': 1,\n",
       " 'grammatical': 1,\n",
       " 'group': 1,\n",
       " 'groups': 2,\n",
       " 'had': 1,\n",
       " 'have': 2,\n",
       " 'he': 3,\n",
       " 'heprp': 1,\n",
       " 'here': 2,\n",
       " 'higher': 1,\n",
       " 'human': 3,\n",
       " 'husband': 1,\n",
       " 'i': 3,\n",
       " 'ice': 2,\n",
       " 'identifies': 1,\n",
       " 'identifying': 1,\n",
       " 'if': 2,\n",
       " 'illinois': 2,\n",
       " 'implementing': 1,\n",
       " 'import': 13,\n",
       " 'important': 1,\n",
       " 'in': 19,\n",
       " 'includes': 1,\n",
       " 'including': 2,\n",
       " 'india': 2,\n",
       " 'individuals': 1,\n",
       " 'indonesia': 2,\n",
       " 'inflectional': 1,\n",
       " 'inflections': 1,\n",
       " 'inflexional': 1,\n",
       " 'information': 2,\n",
       " 'input_str': 12,\n",
       " 'input_stra': 1,\n",
       " 'input_strbeen': 1,\n",
       " 'input_strlower': 1,\n",
       " 'input_strparts': 1,\n",
       " 'input_strstrip': 1,\n",
       " 'input_strthere': 1,\n",
       " 'input_strtranslatestringmaketrans': 1,\n",
       " 'input_strword_tokenizeinput_str': 2,\n",
       " 'inputhe': 1,\n",
       " 'instead': 1,\n",
       " 'interesting': 2,\n",
       " 'into': 3,\n",
       " 'is': 15,\n",
       " 'it': 3,\n",
       " 'its': 6,\n",
       " 'john': 2,\n",
       " 'johnnnp': 1,\n",
       " 'keep': 1,\n",
       " 'keys': 1,\n",
       " 'knowledge': 1,\n",
       " 'lancaster': 1,\n",
       " 'language': 10,\n",
       " 'languages': 1,\n",
       " 'leading': 3,\n",
       " 'lemmatization': 6,\n",
       " 'lemmatizer': 2,\n",
       " 'lemmatizerwordnetlemmatizer': 1,\n",
       " 'length': 1,\n",
       " 'letters': 1,\n",
       " 'lexical': 1,\n",
       " 'libraries': 2,\n",
       " 'library': 1,\n",
       " 'like': 2,\n",
       " 'line': 2,\n",
       " 'links': 1,\n",
       " 'list': 1,\n",
       " 'locations': 2,\n",
       " 'lookedlook': 1,\n",
       " 'lower': 1,\n",
       " 'lowercase': 2,\n",
       " 'lucene': 2,\n",
       " 'machine': 1,\n",
       " 'machinereadable': 1,\n",
       " 'main': 2,\n",
       " 'many': 1,\n",
       " 'mark': 2,\n",
       " 'marks': 2,\n",
       " 'married': 1,\n",
       " 'may': 1,\n",
       " 'mbsp': 2,\n",
       " 'meaning': 1,\n",
       " 'meanings': 1,\n",
       " 'memorybased': 2,\n",
       " 'mentions': 1,\n",
       " 'mice': 1,\n",
       " 'mind': 1,\n",
       " 'mitie': 1,\n",
       " 'more': 3,\n",
       " 'morphological': 1,\n",
       " 'most': 1,\n",
       " 'mouse': 1,\n",
       " 'named': 3,\n",
       " 'namedentity': 4,\n",
       " 'namely': 1,\n",
       " 'names': 1,\n",
       " 'natural': 5,\n",
       " 'ne_chunk': 1,\n",
       " 'ne_chunkpos_tagword_tokenizeinput_str': 1,\n",
       " 'necessary': 1,\n",
       " 'needed': 1,\n",
       " 'ner': 3,\n",
       " 'new': 2,\n",
       " 'newjj': 1,\n",
       " 'nlp': 1,\n",
       " 'nltk': 13,\n",
       " 'nltkregexpparserreg_exp': 1,\n",
       " 'nltkstem': 2,\n",
       " 'nltktokenize': 3,\n",
       " 'normalization': 4,\n",
       " 'not': 4,\n",
       " 'noun': 1,\n",
       " 'nouns': 2,\n",
       " 'np': 4,\n",
       " 'numbers': 7,\n",
       " 'obtained': 1,\n",
       " 'obtaining': 1,\n",
       " 'occurring': 1,\n",
       " 'of': 31,\n",
       " 'off': 1,\n",
       " 'ofin': 1,\n",
       " 'often': 1,\n",
       " 'on': 5,\n",
       " 'open': 1,\n",
       " 'opennlp': 5,\n",
       " 'opensource': 1,\n",
       " 'opposed': 1,\n",
       " 'or': 6,\n",
       " 'order': 1,\n",
       " 'organizations': 2,\n",
       " 'other': 2,\n",
       " 'others': 2,\n",
       " 'output': 12,\n",
       " 'paper': 1,\n",
       " 'parser': 2,\n",
       " 'parsing': 1,\n",
       " 'part': 4,\n",
       " 'particular': 3,\n",
       " 'partofspeech': 2,\n",
       " 'parts': 3,\n",
       " 'pattern': 2,\n",
       " 'people': 1,\n",
       " 'person': 2,\n",
       " 'persons': 1,\n",
       " 'phrases': 1,\n",
       " 'pieces': 1,\n",
       " 'platform': 2,\n",
       " 'population': 2,\n",
       " 'porter': 1,\n",
       " 'porterstemmer': 2,\n",
       " 'pos': 2,\n",
       " 'pos_tag': 1,\n",
       " 'possible': 3,\n",
       " 'post': 1,\n",
       " 'predefined': 1,\n",
       " 'preprocessing': 5,\n",
       " 'presented': 1,\n",
       " 'print': 2,\n",
       " 'printextractorget_collocations_of_lengthinput': 1,\n",
       " 'printinput_str': 1,\n",
       " 'printlemmatizerlemmatizeword': 1,\n",
       " 'printresult': 3,\n",
       " 'printresulttags': 2,\n",
       " 'printstemmerstemword': 1,\n",
       " 'process': 3,\n",
       " 'processing': 2,\n",
       " 'programs': 3,\n",
       " 'pronoun': 1,\n",
       " 'pronouns': 1,\n",
       " 'provides': 1,\n",
       " 'punctuation': 5,\n",
       " 'punctuations': 1,\n",
       " 'python': 6,\n",
       " 'raw': 1,\n",
       " 're': 1,\n",
       " 'ready': 1,\n",
       " 'realworld': 1,\n",
       " 'recognition': 5,\n",
       " 'red': 4,\n",
       " 'reduce': 1,\n",
       " 'reducing': 1,\n",
       " 'refer': 1,\n",
       " 'referring': 1,\n",
       " 'refers': 1,\n",
       " 'reg_exp': 1,\n",
       " 'regular': 1,\n",
       " 'relations': 1,\n",
       " 'relationship': 4,\n",
       " 'relevant': 1,\n",
       " 'removal': 4,\n",
       " 'remove': 10,\n",
       " 'removed': 1,\n",
       " 'removes': 2,\n",
       " 'removing': 5,\n",
       " 'resolution': 8,\n",
       " 'resubrd': 1,\n",
       " 'result': 8,\n",
       " 'resultdraw': 1,\n",
       " 'right': 1,\n",
       " 'root': 1,\n",
       " 'rp': 1,\n",
       " 'rpparseresulttags': 1,\n",
       " 'rules': 1,\n",
       " 's': 2,\n",
       " 'said': 1,\n",
       " 'same': 2,\n",
       " 'sample': 1,\n",
       " 'scikitlearn': 1,\n",
       " 'second': 1,\n",
       " 'sentence': 3,\n",
       " 'sentences': 1,\n",
       " 'set': 1,\n",
       " 'setstopwordswordsenglish': 1,\n",
       " 'sever': 1,\n",
       " 'several': 2,\n",
       " 'shallow': 3,\n",
       " 'sheet': 4,\n",
       " 'should': 1,\n",
       " 'simply': 1,\n",
       " 'sklearnfeature_extractionstop_words': 1,\n",
       " 'smaller': 1,\n",
       " 'so': 2,\n",
       " 'soin': 1,\n",
       " 'some': 2,\n",
       " 'sources': 1,\n",
       " 'spaces': 3,\n",
       " 'spacy': 5,\n",
       " 'spacylangenstop_words': 1,\n",
       " 'sparse': 3,\n",
       " 'speech': 7,\n",
       " 'splitting': 1,\n",
       " 'spouse': 1,\n",
       " 'stanford': 4,\n",
       " 'start': 1,\n",
       " 'stated': 1,\n",
       " 'states': 2,\n",
       " 'statistical': 1,\n",
       " 'stem': 2,\n",
       " 'stemmer': 1,\n",
       " 'stemmers': 1,\n",
       " 'stemming': 12,\n",
       " 'step': 2,\n",
       " 'steps': 4,\n",
       " 'stop': 8,\n",
       " 'stop_words': 3,\n",
       " 'stove': 2,\n",
       " 'stovenn': 1,\n",
       " 'strictly': 1,\n",
       " 'string': 6,\n",
       " 'stringpunctuation': 1,\n",
       " 'strip': 1,\n",
       " 'structure': 1,\n",
       " 'structured': 1,\n",
       " 'such': 2,\n",
       " 'suite': 1,\n",
       " 'summary': 1,\n",
       " 'symbolic': 1,\n",
       " 'symbols': 1,\n",
       " 't': 1,\n",
       " 'table': 5,\n",
       " 'tagger': 1,\n",
       " 'taggers': 1,\n",
       " 'tagging': 4,\n",
       " 'talk': 1,\n",
       " 'talked': 1,\n",
       " 'task': 1,\n",
       " 'tasks': 1,\n",
       " 'techniques': 1,\n",
       " 'television': 2,\n",
       " 'televisionnn': 1,\n",
       " 'temppos_check': 1,\n",
       " 'terms': 3,\n",
       " 'text': 22,\n",
       " 'textblob': 7,\n",
       " 'textblobinput_str': 2,\n",
       " 'textrazor': 1,\n",
       " 'texts': 2,\n",
       " 'than': 1,\n",
       " 'that': 5,\n",
       " 'the': 38,\n",
       " 'thedt': 1,\n",
       " 'their': 1,\n",
       " 'them': 2,\n",
       " 'there': 2,\n",
       " 'these': 2,\n",
       " 'they': 1,\n",
       " 'this': 7,\n",
       " 'time': 1,\n",
       " 'times': 1,\n",
       " 'to': 29,\n",
       " 'together': 1,\n",
       " 'tokenization': 6,\n",
       " 'tokens': 4,\n",
       " 'tool': 1,\n",
       " 'toolkit': 1,\n",
       " 'tools': 13,\n",
       " 'toto': 1,\n",
       " 'transferring': 1,\n",
       " 'translation': 1,\n",
       " 'tree': 1,\n",
       " 'treetagger': 1,\n",
       " 'two': 1,\n",
       " 'type': 1,\n",
       " 'types': 1,\n",
       " 'ucc': 2,\n",
       " 'udt': 4,\n",
       " 'uin': 4,\n",
       " 'ujj': 3,\n",
       " 'understanding': 1,\n",
       " 'united': 2,\n",
       " 'units': 1,\n",
       " 'unn': 5,\n",
       " 'unnp': 1,\n",
       " 'unns': 2,\n",
       " 'unstructured': 1,\n",
       " 'upper': 1,\n",
       " 'urb': 1,\n",
       " 'use': 2,\n",
       " 'used': 2,\n",
       " 'uses': 1,\n",
       " 'using': 11,\n",
       " 'usually': 2,\n",
       " 'uto': 1,\n",
       " 'uvb': 1,\n",
       " 'uvbd': 1,\n",
       " 'uvbg': 1,\n",
       " 'uvbn': 1,\n",
       " 'verb': 1,\n",
       " 'verbs': 2,\n",
       " 'was': 1,\n",
       " 'watson': 1,\n",
       " 'we': 7,\n",
       " 'went': 1,\n",
       " 'wentvbd': 1,\n",
       " 'were': 2,\n",
       " 'werevbd': 1,\n",
       " 'while': 2,\n",
       " 'white': 6,\n",
       " 'whitejj': 1,\n",
       " 'whitespaces': 1,\n",
       " 'will': 3,\n",
       " 'with': 5,\n",
       " 'word': 6,\n",
       " 'word_tokenize': 4,\n",
       " 'word_tokenizeinput_str': 1,\n",
       " 'wordnet': 1,\n",
       " 'wordnetlemmatizer': 1,\n",
       " 'words': 19,\n",
       " 'work': 2,\n",
       " 'works': 1,\n",
       " 'worksvbz': 1,\n",
       " 'would': 2,\n",
       " 'write': 2,\n",
       " 'xrenner': 1,\n",
       " 'yesterday': 1,\n",
       " 'you': 1,\n",
       " 'your': 1}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import operator\n",
    "dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "ja = {}\n",
    "for each in temp:\n",
    "    ja.update({each[0]:each[1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('the', 38),\n",
       "             ('and', 33),\n",
       "             ('of', 31),\n",
       "             ('to', 29),\n",
       "             ('a', 25),\n",
       "             ('text', 22),\n",
       "             ('for', 22),\n",
       "             ('example', 20),\n",
       "             ('in', 19),\n",
       "             ('words', 19),\n",
       "             ('are', 18),\n",
       "             ('from', 17),\n",
       "             ('is', 15),\n",
       "             ('code', 14),\n",
       "             ('tools', 13),\n",
       "             ('import', 13),\n",
       "             ('nltk', 13),\n",
       "             ('input_str', 12),\n",
       "             ('output', 12),\n",
       "             ('stemming', 12),\n",
       "             ('using', 11),\n",
       "             ('language', 10),\n",
       "             ('remove', 10),\n",
       "             ('be', 9),\n",
       "             ('stop', 8),\n",
       "             ('result', 8),\n",
       "             ('resolution', 8),\n",
       "             ('extraction', 8),\n",
       "             ('this', 7),\n",
       "             ('we', 7),\n",
       "             ('numbers', 7),\n",
       "             ('can', 7),\n",
       "             ('', 7),\n",
       "             ('textblob', 7),\n",
       "             ('apache', 7),\n",
       "             ('speech', 7),\n",
       "             ('coreference', 7),\n",
       "             ('or', 6),\n",
       "             ('white', 6),\n",
       "             ('python', 6),\n",
       "             ('string', 6),\n",
       "             ('an', 6),\n",
       "             ('tokenization', 6),\n",
       "             ('described', 6),\n",
       "             ('its', 6),\n",
       "             ('word', 6),\n",
       "             ('lemmatization', 6),\n",
       "             ('chunking', 6),\n",
       "             ('preprocessing', 5),\n",
       "             ('also', 5),\n",
       "             ('with', 5),\n",
       "             ('removing', 5),\n",
       "             ('punctuation', 5),\n",
       "             ('as', 5),\n",
       "             ('table', 5),\n",
       "             ('on', 5),\n",
       "             ('natural', 5),\n",
       "             ('spacy', 5),\n",
       "             ('that', 5),\n",
       "             ('opennlp', 5),\n",
       "             ('unn', 5),\n",
       "             ('recognition', 5),\n",
       "             ('collocation', 5),\n",
       "             ('steps', 4),\n",
       "             ('normalization', 4),\n",
       "             ('not', 4),\n",
       "             ('box', 4),\n",
       "             ('contains', 4),\n",
       "             ('red', 4),\n",
       "             ('balls', 4),\n",
       "             ('removal', 4),\n",
       "             ('tokens', 4),\n",
       "             ('sheet', 4),\n",
       "             ('word_tokenize', 4),\n",
       "             ('algorithm', 4),\n",
       "             ('stanford', 4),\n",
       "             ('corenlp', 4),\n",
       "             ('general', 4),\n",
       "             ('architecture', 4),\n",
       "             ('engineering', 4),\n",
       "             ('part', 4),\n",
       "             ('tagging', 4),\n",
       "             ('examples', 4),\n",
       "             ('uin', 4),\n",
       "             ('udt', 4),\n",
       "             ('np', 4),\n",
       "             ('namedentity', 4),\n",
       "             ('relationship', 4),\n",
       "             ('will', 3),\n",
       "             ('human', 3),\n",
       "             ('all', 3),\n",
       "             ('into', 3),\n",
       "             ('spaces', 3),\n",
       "             ('sparse', 3),\n",
       "             ('terms', 3),\n",
       "             ('particular', 3),\n",
       "             ('by', 3),\n",
       "             ('printresult', 3),\n",
       "             ('leading', 3),\n",
       "             ('process', 3),\n",
       "             ('common', 3),\n",
       "             ('it', 3),\n",
       "             ('possible', 3),\n",
       "             ('programs', 3),\n",
       "             ('stop_words', 3),\n",
       "             ('nltktokenize', 3),\n",
       "             ('i', 3),\n",
       "             ('done', 3),\n",
       "             ('base', 3),\n",
       "             ('more', 3),\n",
       "             ('shallow', 3),\n",
       "             ('gate', 3),\n",
       "             ('dkpro', 3),\n",
       "             ('core', 3),\n",
       "             ('parts', 3),\n",
       "             ('freeling', 3),\n",
       "             ('etc', 3),\n",
       "             ('ujj', 3),\n",
       "             ('adt', 3),\n",
       "             ('forin', 3),\n",
       "             ('sentence', 3),\n",
       "             ('named', 3),\n",
       "             ('ner', 3),\n",
       "             ('he', 3),\n",
       "             ('about', 2),\n",
       "             ('these', 2),\n",
       "             ('processing', 2),\n",
       "             ('after', 2),\n",
       "             ('converting', 2),\n",
       "             ('marks', 2),\n",
       "             ('other', 2),\n",
       "             ('convert', 2),\n",
       "             ('lowercase', 2),\n",
       "             ('biggest', 2),\n",
       "             ('countries', 2),\n",
       "             ('population', 2),\n",
       "             ('china', 2),\n",
       "             ('india', 2),\n",
       "             ('united', 2),\n",
       "             ('states', 2),\n",
       "             ('indonesia', 2),\n",
       "             ('brazil', 2),\n",
       "             ('if', 2),\n",
       "             ('usually', 2),\n",
       "             ('expressions', 2),\n",
       "             ('used', 2),\n",
       "             ('while', 2),\n",
       "             ('b', 2),\n",
       "             ('blue', 2),\n",
       "             ('removes', 2),\n",
       "             ('use', 2),\n",
       "             ('given', 2),\n",
       "             ('others', 2),\n",
       "             ('several', 2),\n",
       "             ('like', 2),\n",
       "             ('do', 2),\n",
       "             ('texts', 2),\n",
       "             ('libraries', 2),\n",
       "             ('platform', 2),\n",
       "             ('building', 2),\n",
       "             ('work', 2),\n",
       "             ('data', 2),\n",
       "             ('print', 2),\n",
       "             ('free', 2),\n",
       "             ('some', 2),\n",
       "             ('stem', 2),\n",
       "             ('form', 2),\n",
       "             ('main', 2),\n",
       "             ('algorithms', 2),\n",
       "             ('nltkstem', 2),\n",
       "             ('porterstemmer', 2),\n",
       "             ('input_strword_tokenizeinput_str', 2),\n",
       "             ('there', 2),\n",
       "             ('forms', 2),\n",
       "             ('get', 2),\n",
       "             ('lemmatizer', 2),\n",
       "             ('pattern', 2),\n",
       "             ('memorybased', 2),\n",
       "             ('parser', 2),\n",
       "             ('mbsp', 2),\n",
       "             ('lucene', 2),\n",
       "             ('illinois', 2),\n",
       "             ('have', 2),\n",
       "             ('pos', 2),\n",
       "             ('partofspeech', 2),\n",
       "             ('aims', 2),\n",
       "             ('each', 2),\n",
       "             ('such', 2),\n",
       "             ('nouns', 2),\n",
       "             ('verbs', 2),\n",
       "             ('adjectives', 2),\n",
       "             ('including', 2),\n",
       "             ('article', 2),\n",
       "             ('write', 2),\n",
       "             ('interesting', 2),\n",
       "             ('easily', 2),\n",
       "             ('textblobinput_str', 2),\n",
       "             ('printresulttags', 2),\n",
       "             ('unns', 2),\n",
       "             ('ucc', 2),\n",
       "             ('them', 2),\n",
       "             ('groups', 2),\n",
       "             ('step', 2),\n",
       "             ('black', 2),\n",
       "             ('television', 2),\n",
       "             ('stove', 2),\n",
       "             ('were', 2),\n",
       "             ('bought', 2),\n",
       "             ('new', 2),\n",
       "             ('apartment', 2),\n",
       "             ('john', 2),\n",
       "             ('s', 2),\n",
       "             ('draw', 2),\n",
       "             ('entity', 2),\n",
       "             ('entities', 2),\n",
       "             ('locations', 2),\n",
       "             ('organizations', 2),\n",
       "             ('so', 2),\n",
       "             ('person', 2),\n",
       "             ('same', 2),\n",
       "             ('andrew', 2),\n",
       "             ('would', 2),\n",
       "             ('found', 2),\n",
       "             ('here', 2),\n",
       "             ('ice', 2),\n",
       "             ('line', 2),\n",
       "             ('information', 2),\n",
       "             ('eg', 2),\n",
       "             ('mark', 2),\n",
       "             ('paper', 1),\n",
       "             ('talk', 1),\n",
       "             ('basic', 1),\n",
       "             ('needed', 1),\n",
       "             ('transferring', 1),\n",
       "             ('machinereadable', 1),\n",
       "             ('format', 1),\n",
       "             ('further', 1),\n",
       "             ('discuss', 1),\n",
       "             ('obtained', 1),\n",
       "             ('start', 1),\n",
       "             ('includes', 1),\n",
       "             ('letters', 1),\n",
       "             ('lower', 1),\n",
       "             ('upper', 1),\n",
       "             ('case', 1),\n",
       "             ('punctuations', 1),\n",
       "             ('accent', 1),\n",
       "             ('diacritics', 1),\n",
       "             ('expanding', 1),\n",
       "             ('abbreviations', 1),\n",
       "             ('canonicalization', 1),\n",
       "             ('describe', 1),\n",
       "             ('detail', 1),\n",
       "             ('below', 1),\n",
       "             ('input_strlower', 1),\n",
       "             ('printinput_str', 1),\n",
       "             ('they', 1),\n",
       "             ('relevant', 1),\n",
       "             ('your', 1),\n",
       "             ('analyses', 1),\n",
       "             ('regular', 1),\n",
       "             ('re', 1),\n",
       "             ('resubrd', 1),\n",
       "             ('following', 1),\n",
       "             ('set', 1),\n",
       "             ('symbols', 1),\n",
       "             ('_', 1),\n",
       "             ('sample', 1),\n",
       "             ('input_strtranslatestringmaketrans', 1),\n",
       "             ('stringpunctuation', 1),\n",
       "             ('whitespaces', 1),\n",
       "             ('ending', 1),\n",
       "             ('you', 1),\n",
       "             ('strip', 1),\n",
       "             ('function', 1),\n",
       "             ('t', 1),\n",
       "             ('examplet', 1),\n",
       "             ('input_strstrip', 1),\n",
       "             ('splitting', 1),\n",
       "             ('smaller', 1),\n",
       "             ('pieces', 1),\n",
       "             ('called', 1),\n",
       "             ('considered', 1),\n",
       "             ('implementing', 1),\n",
       "             ('most', 1),\n",
       "             ('carry', 1),\n",
       "             ('important', 1),\n",
       "             ('meaning', 1),\n",
       "             ('removed', 1),\n",
       "             ('toolkit', 1),\n",
       "             ('suite', 1),\n",
       "             ('symbolic', 1),\n",
       "             ('statistical', 1),\n",
       "             ('setstopwordswordsenglish', 1),\n",
       "             ('word_tokenizeinput_str', 1),\n",
       "             ('scikitlearn', 1),\n",
       "             ('tool', 1),\n",
       "             ('provides', 1),\n",
       "             ('list', 1),\n",
       "             ('sklearnfeature_extractionstop_words', 1),\n",
       "             ('english_stop_words', 1),\n",
       "             ('opensource', 1),\n",
       "             ('library', 1),\n",
       "             ('spacylangenstop_words', 1),\n",
       "             ('cases', 1),\n",
       "             ('necessary', 1),\n",
       "             ('task', 1),\n",
       "             ('techniques', 1),\n",
       "             ('considering', 1),\n",
       "             ('any', 1),\n",
       "             ('group', 1),\n",
       "             ('chosen', 1),\n",
       "             ('reducing', 1),\n",
       "             ('their', 1),\n",
       "             ('root', 1),\n",
       "             ('booksbook', 1),\n",
       "             ('lookedlook', 1),\n",
       "             ('two', 1),\n",
       "             ('porter', 1),\n",
       "             ('morphological', 1),\n",
       "             ('inflexional', 1),\n",
       "             ('endings', 1),\n",
       "             ('lancaster', 1),\n",
       "             ('aggressive', 1),\n",
       "             ('stemmers', 1),\n",
       "             ('stemmer', 1),\n",
       "             ('input_strthere', 1),\n",
       "             ('types', 1),\n",
       "             ('printstemmerstemword', 1),\n",
       "             ('sever', 1),\n",
       "             ('type', 1),\n",
       "             ('aim', 1),\n",
       "             ('reduce', 1),\n",
       "             ('inflectional', 1),\n",
       "             ('opposed', 1),\n",
       "             ('does', 1),\n",
       "             ('simply', 1),\n",
       "             ('chop', 1),\n",
       "             ('off', 1),\n",
       "             ('inflections', 1),\n",
       "             ('instead', 1),\n",
       "             ('uses', 1),\n",
       "             ('lexical', 1),\n",
       "             ('knowledge', 1),\n",
       "             ('bases', 1),\n",
       "             ('correct', 1),\n",
       "             ('presented', 1),\n",
       "             ('above', 1),\n",
       "             ('wordnet', 1),\n",
       "             ('gensim', 1),\n",
       "             ('wordnetlemmatizer', 1),\n",
       "             ('lemmatizerwordnetlemmatizer', 1),\n",
       "             ('input_strbeen', 1),\n",
       "             ('had', 1),\n",
       "             ('languages', 1),\n",
       "             ('cities', 1),\n",
       "             ('mice', 1),\n",
       "             ('printlemmatizerlemmatizeword', 1),\n",
       "             ('city', 1),\n",
       "             ('mouse', 1),\n",
       "             ('assign', 1),\n",
       "             ('based', 1),\n",
       "             ('definition', 1),\n",
       "             ('context', 1),\n",
       "             ('many', 1),\n",
       "             ('containing', 1),\n",
       "             ('taggers', 1),\n",
       "             ('tagger', 1),\n",
       "             ('input_strparts', 1),\n",
       "             ('uto', 1),\n",
       "             ('uvb', 1),\n",
       "             ('uvbg', 1),\n",
       "             ('urb', 1),\n",
       "             ('parsing', 1),\n",
       "             ('identifies', 1),\n",
       "             ('constituent', 1),\n",
       "             ('sentences', 1),\n",
       "             ('links', 1),\n",
       "             ('higher', 1),\n",
       "             ('order', 1),\n",
       "             ('units', 1),\n",
       "             ('discrete', 1),\n",
       "             ('grammatical', 1),\n",
       "             ('meanings', 1),\n",
       "             ('noun', 1),\n",
       "             ('phrases', 1),\n",
       "             ('verb', 1),\n",
       "             ('treetagger', 1),\n",
       "             ('chunker', 1),\n",
       "             ('first', 1),\n",
       "             ('determine', 1),\n",
       "             ('input_stra', 1),\n",
       "             ('uvbd', 1),\n",
       "             ('uvbn', 1),\n",
       "             ('unnp', 1),\n",
       "             ('second', 1),\n",
       "             ('reg_exp', 1),\n",
       "             ('dtjjnn', 1),\n",
       "             ('rp', 1),\n",
       "             ('nltkregexpparserreg_exp', 1),\n",
       "             ('rpparseresulttags', 1),\n",
       "             ('blackjj', 1),\n",
       "             ('televisionnn', 1),\n",
       "             ('andcc', 1),\n",
       "             ('whitejj', 1),\n",
       "             ('stovenn', 1),\n",
       "             ('werevbd', 1),\n",
       "             ('boughtvbn', 1),\n",
       "             ('thedt', 1),\n",
       "             ('newjj', 1),\n",
       "             ('apartmentnn', 1),\n",
       "             ('ofin', 1),\n",
       "             ('johnnnp', 1),\n",
       "             ('tree', 1),\n",
       "             ('structure', 1),\n",
       "             ('resultdraw', 1),\n",
       "             ('find', 1),\n",
       "             ('classify', 1),\n",
       "             ('predefined', 1),\n",
       "             ('categories', 1),\n",
       "             ('names', 1),\n",
       "             ('persons', 1),\n",
       "             ('times', 1),\n",
       "             ('gateannie', 1),\n",
       "             ('mitie', 1),\n",
       "             ('watson', 1),\n",
       "             ('understanding', 1),\n",
       "             ('textrazor', 1),\n",
       "             ('pos_tag', 1),\n",
       "             ('ne_chunk', 1),\n",
       "             ('bill', 1),\n",
       "             ('works', 1),\n",
       "             ('apple', 1),\n",
       "             ('went', 1),\n",
       "             ('boston', 1),\n",
       "             ('conference', 1),\n",
       "             ('ne_chunkpos_tagword_tokenizeinput_str', 1),\n",
       "             ('billnnp', 1),\n",
       "             ('worksvbz', 1),\n",
       "             ('applennp', 1),\n",
       "             ('soin', 1),\n",
       "             ('heprp', 1),\n",
       "             ('wentvbd', 1),\n",
       "             ('toto', 1),\n",
       "             ('gpe', 1),\n",
       "             ('bostonnnp', 1),\n",
       "             ('conferencenn', 1),\n",
       "             ('anaphora', 1),\n",
       "             ('pronouns', 1),\n",
       "             ('referring', 1),\n",
       "             ('should', 1),\n",
       "             ('connected', 1),\n",
       "             ('right', 1),\n",
       "             ('individuals', 1),\n",
       "             ('finds', 1),\n",
       "             ('mentions', 1),\n",
       "             ('refer', 1),\n",
       "             ('realworld', 1),\n",
       "             ('said', 1),\n",
       "             ('buy', 1),\n",
       "             ('car', 1),\n",
       "             ('pronoun', 1),\n",
       "             ('refers', 1),\n",
       "             ('namely', 1),\n",
       "             ('open', 1),\n",
       "             ('calais', 1),\n",
       "             ('xrenner', 1),\n",
       "             ('collocations', 1),\n",
       "             ('combinations', 1),\n",
       "             ('occurring', 1),\n",
       "             ('together', 1),\n",
       "             ('often', 1),\n",
       "             ('than', 1),\n",
       "             ('expected', 1),\n",
       "             ('chance', 1),\n",
       "             ('break', 1),\n",
       "             ('rules', 1),\n",
       "             ('time', 1),\n",
       "             ('conclusion', 1),\n",
       "             ('keep', 1),\n",
       "             ('mind', 1),\n",
       "             ('ready', 1),\n",
       "             ('inputhe', 1),\n",
       "             ('chazz', 1),\n",
       "             ('duel', 1),\n",
       "             ('keys', 1),\n",
       "             ('collocationextractor', 1),\n",
       "             ('extractor', 1),\n",
       "             ('collocationextractorwith_collocation_pipelinet', 1),\n",
       "             ('bing_key', 1),\n",
       "             ('temppos_check', 1),\n",
       "             ('false', 1),\n",
       "             ('printextractorget_collocations_of_lengthinput', 1),\n",
       "             ('length', 1),\n",
       "             ('allows', 1),\n",
       "             ('obtaining', 1),\n",
       "             ('structured', 1),\n",
       "             ('unstructured', 1),\n",
       "             ('sources', 1),\n",
       "             ('raw', 1),\n",
       "             ('strictly', 1),\n",
       "             ('stated', 1),\n",
       "             ('identifying', 1),\n",
       "             ('relations', 1),\n",
       "             ('acquisition', 1),\n",
       "             ('spouse', 1),\n",
       "             ('employment', 1),\n",
       "             ('among', 1),\n",
       "             ('people', 1),\n",
       "             ('emily', 1),\n",
       "             ('married', 1),\n",
       "             ('yesterday', 1),\n",
       "             ('extract', 1),\n",
       "             ('emilys', 1),\n",
       "             ('husband', 1),\n",
       "             ('summary', 1),\n",
       "             ('post', 1),\n",
       "             ('talked', 1),\n",
       "             ('discussed', 1),\n",
       "             ('comparative', 1),\n",
       "             ('was', 1),\n",
       "             ('created', 1),\n",
       "             ('may', 1),\n",
       "             ('complicated', 1),\n",
       "             ('nlp', 1),\n",
       "             ('tasks', 1),\n",
       "             ('machine', 1),\n",
       "             ('translation', 1),\n",
       "             ('generation', 1)])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "for key, value in OrderedDict(sorted(dic.items(),key=lambda kv: kv[1], reverse=True))\n",
    "    print(key,\" - \",value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
