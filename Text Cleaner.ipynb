{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# v 0.4.16\n",
    "import regex as re,string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords \n",
    "stop_words = set(stopwords.words('english'))\n",
    "from nltk.stem import SnowballStemmer\n",
    "stemmer= SnowballStemmer('english')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "import os.path\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#removes all the blank line from the text file\n",
    "#returns list\n",
    "def clear_blank_lines(file):\n",
    "    return document(file).clear_blank_lines().data\n",
    "\n",
    "# it removes \".\\n\" from every element by default\n",
    "# can be used to strip by second argument\n",
    "def strip_all(file,x='.\\n'):\n",
    "    return document(file).strip_all(x).data\n",
    "\n",
    "\n",
    "# converts each character to lowercase\n",
    "def lower_all(file):\n",
    "    return document(file).lower_all().data\n",
    "\n",
    "# removes numbers detected anywhere in the data\n",
    "def remove_numbers(file):\n",
    "    return document(file).remove_numbers().data\n",
    "\n",
    "# removes punctuations detected anywhere in the data\n",
    "def remove_symbols(file):\n",
    "    return document(file).remove_symbols().data\n",
    "\n",
    "# it will remove stop words and return a list of list of words  \n",
    "def remove_stpwrds(file,op='sents'):\n",
    "    return document(file).remove_stpwrds().data\n",
    "\n",
    "#for tokenization\n",
    "def token_it(file):\n",
    "    return document(file).token_it()\n",
    "\n",
    "# reduces each word to its stem work like, dogs to dog\n",
    "def stemming(file):\n",
    "    return document(file).stemming().data\n",
    "    \n",
    "# gets the root word for each word\n",
    "def lemming(file):\n",
    "    return document(file).lemming().data\n",
    "    \n",
    "\n",
    "def main_cleaner(file,op = 'sents'):\n",
    "    return document(file).main_cleaner().data\n",
    "\n",
    "def formating(block):\n",
    "    return [' '.join(each) for each in block]\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class document:\n",
    "    file_name = ''\n",
    "    data = []\n",
    "    words = []\n",
    "    tokens = []\n",
    "    count_sentences = 0\n",
    "    count_words = 0\n",
    "    each_word_count = {}\n",
    "    \n",
    "    def __init__(self,fname=''):\n",
    "        \n",
    "        if fname:            \n",
    "            self.data,fame =  self._reader(fname)\n",
    "            self.words = [each.split(' ') for each in self.clear_blank_lines().lower_all().strip_all().remove_numbers().remove_symbols()]\n",
    "            self.count_words = len([word for sent in self.words for word in sent])\n",
    "            self.count_sentences = sum(each.count('.') for each in self.data)\n",
    "            temp = self._flatlist(self.words)\n",
    "            self.each_word_count = {x:temp.count(x) for x in temp}\n",
    "            self.file_name = fame.split('\\\\')[-1]\n",
    "        else:\n",
    "            pass  \n",
    "        \n",
    "    def _flatlist(self,lis):\n",
    "        return [word for sent in lis for word in sent]\n",
    "        \n",
    "    def __repr__(self):\n",
    "         return '\\n'.join(map(str, self.data))\n",
    "\n",
    "    def __str__(self):\n",
    "        return '\\n'.join(map(str, self.data))\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for i in self.data: yield i\n",
    "    \n",
    "    def copy(self):\n",
    "        temp = copy.deepcopy(self)       \n",
    "        return temp\n",
    "\n",
    "    # checks whether it has file path as argument\n",
    "    def _file_or_not(self,arg):\n",
    "        if os.path.isfile(arg):\n",
    "            return True\n",
    "        else:\n",
    "            return False,\"only supports .txt for now\"\n",
    "\n",
    "    # this reader is flexible enough to process file or will return the data if list is being passed to the function.\n",
    "    def _reader(self,file):\n",
    "        if type(file)==str and self._file_or_not(file)==True:\n",
    "            with open(file,'r') as f:\n",
    "                return f.readlines(),file\n",
    "\n",
    "        elif type(file)==str:\n",
    "            return file.split('\\n'),''\n",
    "\n",
    "        else: return file,''\n",
    "\n",
    "\n",
    "    \n",
    "    #removes all the blank line from the text file\n",
    "    #returns list\n",
    "    def clear_blank_lines(self,inplace=False):\n",
    "        if not inplace: self = self.copy()\n",
    "        self.data =  list(filter(str.strip,[each.rstrip() for each in self.data]))\n",
    "        return self\n",
    "\n",
    "    \n",
    "    # it removes \".\\n\" from every element by default\n",
    "    # can be used to strip by second argument\n",
    "    def strip_all(self,x='.\\n',inplace=False):\n",
    "        if not inplace: self = self.copy()\n",
    "        self.data = [re.sub(r'\\s+',' ',each.strip(x)) for each in self.data]\n",
    "        return self\n",
    "\n",
    "\n",
    "    # converts each character to lowercase\n",
    "    def lower_all(self,inplace=False):\n",
    "        if not inplace: self = self.copy()\n",
    "        self.data = [each.lower() for each in self.data]\n",
    "        return self\n",
    "\n",
    "    # removes numbers detected anywhere in the data\n",
    "    def remove_numbers(self,inplace=False):\n",
    "        if not inplace: self = self.copy()\n",
    "        self.data = [re.sub(r'[0-9]+', '',(each)) for each in self.data]\n",
    "        return self\n",
    "\n",
    "    # removes punctuations detected anywhere in the data\n",
    "    def remove_symbols(self,inplace=False):\n",
    "        if not inplace: self = self.copy()\n",
    "        self.data = [re.sub(r'[^\\w\\s]','',each) for each in self.data]\n",
    "        return self.strip_all()\n",
    "\n",
    "    \n",
    "    # it will remove stop words and return a list of list of words  \n",
    "    def remove_stpwrds(self,inplace=False):\n",
    "        if not inplace: self = self.copy()\n",
    "        self.words = [[w for w in each.split() if not w in stop_words] for each in self.data] \n",
    "    \n",
    "        self.data = formating(self.words)        \n",
    "        return self\n",
    "        \n",
    "\n",
    "    #for tokenization this function can't be use as object \n",
    "    def token_it(self):\n",
    "        self.tokens = [word_tokenize(each) for each in self.data]\n",
    "        return self.tokens\n",
    "\n",
    "    # reduces each word to its stem work like, dogs to dog\n",
    "    def stemming(self,inplace=False):\n",
    "        if not inplace: self = self.copy()\n",
    "        self.data = formating([[stemmer.stem(word) for word in each.split()] for each in self.data])\n",
    "        return self\n",
    "\n",
    "    # gets the root word for each word\n",
    "    def lemming(self,inplace=False):\n",
    "        if not inplace: self = self.copy()\n",
    "        self.data = formating([[lemmatizer.lemmatize(word) for word in each.split()] for each in self.data])\n",
    "        return self\n",
    "\n",
    "\n",
    "    def main_cleaner(self,op = 'sents',inplace=False):\n",
    "        if not inplace: self = self.copy()\n",
    "            \n",
    "        # this is the basic cleaning which operates with each line\n",
    "        part1 = self.clear_blank_lines().strip_all().lower_all().remove_numbers().remove_symbols()\n",
    "\n",
    "        # this is the advanced cleaning which operates with each word\n",
    "        part2 = part1.lemming().remove_stpwrds()\n",
    "\n",
    "        if op== 'sents':\n",
    "            return part2\n",
    "\n",
    "        if op== 'words':\n",
    "            return [word for sent in part2.data for word in sent.split()]        \n",
    "\n",
    "        if op not in ('sents','words'):\n",
    "            return \"value of option is not valid, try 'sents' or 'words' instead\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textcleaner\n",
      "Requirement already satisfied, skipping upgrade: NLTK in e:\\anaconda\\lib\\site-packages (from textcleaner) (3.2.4)\n",
      "Requirement already satisfied, skipping upgrade: REGEX in e:\\anaconda\\lib\\site-packages (from textcleaner) (2018.11.22)\n",
      "Requirement already satisfied, skipping upgrade: six in e:\\anaconda\\lib\\site-packages (from NLTK->textcleaner) (1.11.0)\n",
      "Installing collected packages: textcleaner\n",
      "  Found existing installation: textcleaner 0.4.14\n",
      "    Uninstalling textcleaner-0.4.14:\n",
      "      Successfully uninstalled textcleaner-0.4.14\n",
      "  Rolling back uninstall of textcleaner\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception:\n",
      "Traceback (most recent call last):\n",
      "  File \"e:\\anaconda\\lib\\site-packages\\pip\\_internal\\cli\\base_command.py\", line 143, in main\n",
      "    status = self.run(options, args)\n",
      "  File \"e:\\anaconda\\lib\\site-packages\\pip\\_internal\\commands\\install.py\", line 366, in run\n",
      "    use_user_site=options.use_user_site,\n",
      "  File \"e:\\anaconda\\lib\\site-packages\\pip\\_internal\\req\\__init__.py\", line 49, in install_given_reqs\n",
      "    **kwargs\n",
      "  File \"e:\\anaconda\\lib\\site-packages\\pip\\_internal\\req\\req_install.py\", line 760, in install\n",
      "    use_user_site=use_user_site, pycompile=pycompile,\n",
      "  File \"e:\\anaconda\\lib\\site-packages\\pip\\_internal\\req\\req_install.py\", line 382, in move_wheel_files\n",
      "    warn_script_location=warn_script_location,\n",
      "  File \"e:\\anaconda\\lib\\site-packages\\pip\\_internal\\wheel.py\", line 215, in move_wheel_files\n",
      "    prefix=prefix,\n",
      "  File \"e:\\anaconda\\lib\\site-packages\\pip\\_internal\\locations.py\", line 153, in distutils_scheme\n",
      "    d.parse_config_files()\n",
      "  File \"e:\\anaconda\\lib\\distutils\\dist.py\", line 395, in parse_config_files\n",
      "    parser.read(filename)\n",
      "  File \"e:\\anaconda\\lib\\configparser.py\", line 697, in read\n",
      "    self._read(fp, filename)\n",
      "  File \"e:\\anaconda\\lib\\configparser.py\", line 1080, in _read\n",
      "    raise MissingSectionHeaderError(fpname, lineno, line)\n",
      "configparser.MissingSectionHeaderError: File contains no section headers.\n",
      "file: 'setup.cfg', line: 1\n",
      "'description-file = README.md\\n'\n"
     ]
    }
   ],
   "source": [
    "!pip install textcleaner --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import textcleaner as tc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
