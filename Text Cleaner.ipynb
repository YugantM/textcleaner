{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# v 0.3.1\n",
    "import regex as re,string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords \n",
    "stop_words = set(stopwords.words('english'))\n",
    "from nltk.stem import SnowballStemmer\n",
    "stemmer= SnowballStemmer('english')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "import os.path\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#removes all the blank line from the text file\n",
    "#returns list\n",
    "def clear_blank_lines(file):\n",
    "    return document(file).clear_blank_lines().data\n",
    "\n",
    "# it removes \".\\n\" from every element by default\n",
    "# can be used to strip by second argument\n",
    "def strip_all(file,x='.\\n'):\n",
    "    return document(file).strip_all(x).data\n",
    "\n",
    "\n",
    "# converts each character to lowercase\n",
    "def lower_all(file):\n",
    "    return document(file).lower_all().data\n",
    "\n",
    "# removes numbers detected anywhere in the data\n",
    "def remove_numbers(file):\n",
    "    return document(file).remove_numbers().data\n",
    "\n",
    "# removes punctuations detected anywhere in the data\n",
    "def remove_symbols(file):\n",
    "    return document(file).remove_symbols().data\n",
    "\n",
    "# it will remove stop words and return a list of list of words  \n",
    "def remove_stpwrds(file,op='sents'):\n",
    "    return document(file).remove_stpwrds().data\n",
    "\n",
    "#for tokenization\n",
    "def token_it(file):\n",
    "    return document(file).token_it()\n",
    "\n",
    "# reduces each word to its stem work like, dogs to dog\n",
    "def stemming(file):\n",
    "    return document(file).stemming().data\n",
    "    \n",
    "# gets the root word for each word\n",
    "def lemming(file):\n",
    "    return document(file).lemming().data\n",
    "    \n",
    "\n",
    "def main_cleaner(file,op = 'sents'):\n",
    "    return document(file).main_cleaner().data\n",
    "\n",
    "def formating(block):\n",
    "    return [' '.join(each) for each in block]\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class document:\n",
    "    file_name = ''\n",
    "    data = []\n",
    "    words = []\n",
    "    tokens = []\n",
    "    count_sentences = 0\n",
    "    count_words = 0\n",
    "    each_word_count = {}\n",
    "    \n",
    "    def __init__(self,fname=''):\n",
    "        \n",
    "        if fname:            \n",
    "            self.data,fame =  self._reader(fname)\n",
    "            self.words = [each.split(' ') for each in self.clear_blank_lines().strip_all().remove_numbers().remove_symbols()]\n",
    "            self.count_words = len([word for sent in self.words for word in sent])\n",
    "            self.count_sentences = sum(each.count('.') for each in self.data)\n",
    "            self.each_word_count = _flatlist(self.words)\n",
    "            self.file_name = fame.split('\\\\')[-1]\n",
    "        else:\n",
    "            pass  \n",
    "        \n",
    "    def _flatlist(self,lis):\n",
    "        return [word for sent in lis for word in sent]\n",
    "        \n",
    "    def __repr__(self):\n",
    "         return '\\n'.join(map(str, self.data))\n",
    "\n",
    "    def __str__(self):\n",
    "        return '\\n'.join(map(str, self.data))\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for i in self.data: yield i\n",
    "    \n",
    "    def copy(self):\n",
    "        temp = copy.deepcopy(self)       \n",
    "        return temp\n",
    "\n",
    "    # checks whether it has file path as argument\n",
    "    def _file_or_not(self,arg):\n",
    "        if os.path.isfile(arg):\n",
    "            return True\n",
    "        else:\n",
    "            return False,\"only supports .txt for now\"\n",
    "\n",
    "    # this reader is flexible enough to process file or will return the data if list is being passed to the function.\n",
    "    def _reader(self,file):\n",
    "        if type(file)==str and self._file_or_not(file)==True:\n",
    "            with open(file,'r') as f:\n",
    "                return f.readlines(),file\n",
    "\n",
    "        elif type(file)==str:\n",
    "            return file.split('\\n'),''\n",
    "\n",
    "        else: return file,''\n",
    "\n",
    "\n",
    "    \n",
    "    #removes all the blank line from the text file\n",
    "    #returns list\n",
    "    def clear_blank_lines(self,inplace=False):\n",
    "        if not inplace: self = self.copy()\n",
    "        self.data =  list(filter(str.strip,[each.rstrip() for each in self.data]))\n",
    "        return self\n",
    "\n",
    "    \n",
    "    # it removes \".\\n\" from every element by default\n",
    "    # can be used to strip by second argument\n",
    "    def strip_all(self,x='.\\n',inplace=False):\n",
    "        if not inplace: self = self.copy()\n",
    "        self.data = [re.sub(r'\\s+',' ',each.strip(x)) for each in self.data]\n",
    "        return self\n",
    "\n",
    "\n",
    "    # converts each character to lowercase\n",
    "    def lower_all(self,inplace=False):\n",
    "        if not inplace: self = self.copy()\n",
    "        self.data = [each.lower() for each in self.data]\n",
    "        return self\n",
    "\n",
    "    # removes numbers detected anywhere in the data\n",
    "    def remove_numbers(self,inplace=False):\n",
    "        if not inplace: self = self.copy()\n",
    "        self.data = [re.sub(r'[0-9]+', '',(each)) for each in self.data]\n",
    "        return self\n",
    "\n",
    "    # removes punctuations detected anywhere in the data\n",
    "    def remove_symbols(self,inplace=False):\n",
    "        if not inplace: self = self.copy()\n",
    "        self.data = [re.sub(r'[^\\w\\s]','',each) for each in self.data]\n",
    "        return self.strip_all()\n",
    "\n",
    "    \n",
    "    # it will remove stop words and return a list of list of words  \n",
    "    def remove_stpwrds(self,inplace=False):\n",
    "        if not inplace: self = self.copy()\n",
    "        self.words = [[w for w in each.split() if not w in stop_words] for each in self.data] \n",
    "    \n",
    "        self.data = formating(self.words)        \n",
    "        return self\n",
    "        \n",
    "\n",
    "    #for tokenization this function can't be use as object \n",
    "    def token_it(self):\n",
    "        self.tokens = [word_tokenize(each) for each in self.data]\n",
    "        return self.tokens\n",
    "\n",
    "    # reduces each word to its stem work like, dogs to dog\n",
    "    def stemming(self,inplace=False):\n",
    "        if not inplace: self = self.copy()\n",
    "        self.data = formating([[stemmer.stem(word) for word in each.split()] for each in self.data])\n",
    "        return self\n",
    "\n",
    "    # gets the root word for each word\n",
    "    def lemming(self,inplace=False):\n",
    "        if not inplace: self = self.copy()\n",
    "        self.data = formating([[lemmatizer.lemmatize(word) for word in each.split()] for each in self.data])\n",
    "        return self\n",
    "\n",
    "\n",
    "    def main_cleaner(self,op = 'sents',inplace=False):\n",
    "        if not inplace: self = self.copy()\n",
    "            \n",
    "        # this is the basic cleaning which operates with each line\n",
    "        part1 = self.clear_blank_lines().strip_all().lower_all().remove_numbers().remove_symbols()\n",
    "\n",
    "        # this is the advanced cleaning which operates with each word\n",
    "        part2 = part1.lemming().remove_stpwrds()\n",
    "\n",
    "        if op== 'sents':\n",
    "            return part2\n",
    "\n",
    "        if op== 'words':\n",
    "            return [word for sent in part2.data for word in sent.split()]        \n",
    "\n",
    "        if op not in ('sents','words'):\n",
    "            return \"value of option is not valid, try 'sents' or 'words' instead\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '_flatlist' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-97-35489fab5f7b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtemp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdocument\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'file.txt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-96-ca6b50f91319>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, fname)\u001b[0m\n\u001b[0;32m     15\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msent\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount_sentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meach\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'.'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0meach\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meach_word_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_flatlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfile_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\\\'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name '_flatlist' is not defined"
     ]
    }
   ],
   "source": [
    "temp = document('file.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['In',\n",
       "  'this',\n",
       "  'paper',\n",
       "  'we',\n",
       "  'will',\n",
       "  'talk',\n",
       "  'about',\n",
       "  'the',\n",
       "  'basic',\n",
       "  'steps',\n",
       "  'of',\n",
       "  'text',\n",
       "  'preprocessing',\n",
       "  'These',\n",
       "  'steps',\n",
       "  'are',\n",
       "  'needed',\n",
       "  'for',\n",
       "  'transferring',\n",
       "  'text',\n",
       "  'from',\n",
       "  'human',\n",
       "  'language',\n",
       "  'to',\n",
       "  'machinereadable',\n",
       "  'format',\n",
       "  'for',\n",
       "  'further',\n",
       "  'processing',\n",
       "  'We',\n",
       "  'will',\n",
       "  'also',\n",
       "  'discuss',\n",
       "  'text',\n",
       "  'preprocessing',\n",
       "  'tools'],\n",
       " ['After',\n",
       "  'a',\n",
       "  'text',\n",
       "  'is',\n",
       "  'obtained',\n",
       "  'we',\n",
       "  'start',\n",
       "  'with',\n",
       "  'text',\n",
       "  'normalization',\n",
       "  'Text',\n",
       "  'normalization',\n",
       "  'includes'],\n",
       " ['converting', 'all', 'letters', 'to', 'lower', 'or', 'upper', 'case'],\n",
       " ['converting', 'numbers', 'into', 'words', 'or', 'removing', 'numbers'],\n",
       " ['removing', 'punctuations', 'accent', 'marks', 'and', 'other', 'diacritics'],\n",
       " ['removing', 'white', 'spaces'],\n",
       " ['expanding', 'abbreviations'],\n",
       " ['removing',\n",
       "  'stop',\n",
       "  'words',\n",
       "  'sparse',\n",
       "  'terms',\n",
       "  'and',\n",
       "  'particular',\n",
       "  'words'],\n",
       " ['text', 'canonicalization'],\n",
       " ['We',\n",
       "  'will',\n",
       "  'describe',\n",
       "  'text',\n",
       "  'normalization',\n",
       "  'steps',\n",
       "  'in',\n",
       "  'detail',\n",
       "  'below'],\n",
       " ['Convert', 'text', 'to', 'lowercase'],\n",
       " ['Example', 'Convert', 'text', 'to', 'lowercase'],\n",
       " ['Python', 'code'],\n",
       " ['input_str',\n",
       "  'The',\n",
       "  'biggest',\n",
       "  'countries',\n",
       "  'by',\n",
       "  'population',\n",
       "  'in',\n",
       "  'are',\n",
       "  'China',\n",
       "  'India',\n",
       "  'United',\n",
       "  'States',\n",
       "  'Indonesia',\n",
       "  'and',\n",
       "  'Brazil'],\n",
       " ['input_str', 'input_strlower'],\n",
       " ['printinput_str'],\n",
       " ['Output'],\n",
       " ['the',\n",
       "  'biggest',\n",
       "  'countries',\n",
       "  'by',\n",
       "  'population',\n",
       "  'in',\n",
       "  'are',\n",
       "  'china',\n",
       "  'india',\n",
       "  'united',\n",
       "  'states',\n",
       "  'indonesia',\n",
       "  'and',\n",
       "  'brazil'],\n",
       " ['Remove', 'numbers'],\n",
       " ['Remove',\n",
       "  'numbers',\n",
       "  'if',\n",
       "  'they',\n",
       "  'are',\n",
       "  'not',\n",
       "  'relevant',\n",
       "  'to',\n",
       "  'your',\n",
       "  'analyses',\n",
       "  'Usually',\n",
       "  'regular',\n",
       "  'expressions',\n",
       "  'are',\n",
       "  'used',\n",
       "  'to',\n",
       "  'remove',\n",
       "  'numbers'],\n",
       " ['Example', 'Numbers', 'removing'],\n",
       " ['Python', 'code'],\n",
       " ['import', 're'],\n",
       " ['input_str',\n",
       "  'Box',\n",
       "  'A',\n",
       "  'contains',\n",
       "  'red',\n",
       "  'and',\n",
       "  'white',\n",
       "  'balls',\n",
       "  'while',\n",
       "  'Box',\n",
       "  'B',\n",
       "  'contains',\n",
       "  'red',\n",
       "  'and',\n",
       "  'blue',\n",
       "  'balls'],\n",
       " ['result', 'resubrd', 'input_str'],\n",
       " ['printresult'],\n",
       " ['Output'],\n",
       " ['Box',\n",
       "  'A',\n",
       "  'contains',\n",
       "  'red',\n",
       "  'and',\n",
       "  'white',\n",
       "  'balls',\n",
       "  'while',\n",
       "  'Box',\n",
       "  'B',\n",
       "  'contains',\n",
       "  'red',\n",
       "  'and',\n",
       "  'blue',\n",
       "  'balls'],\n",
       " ['Remove', 'punctuation'],\n",
       " ['The', 'following', 'code', 'removes', 'this', 'set', 'of', 'symbols', '_'],\n",
       " ['Example', 'Punctuation', 'removal'],\n",
       " ['Python', 'code'],\n",
       " ['import', 'string'],\n",
       " ['input_str',\n",
       "  'This',\n",
       "  'is',\n",
       "  'an',\n",
       "  'example',\n",
       "  'of',\n",
       "  'string',\n",
       "  'with',\n",
       "  'punctuation',\n",
       "  'Sample',\n",
       "  'string'],\n",
       " ['result', 'input_strtranslatestringmaketrans', 'stringpunctuation'],\n",
       " ['printresult'],\n",
       " ['Output'],\n",
       " ['This', 'is', 'an', 'example', 'of', 'string', 'with', 'punctuation'],\n",
       " ['Remove', 'whitespaces'],\n",
       " ['To',\n",
       "  'remove',\n",
       "  'leading',\n",
       "  'and',\n",
       "  'ending',\n",
       "  'spaces',\n",
       "  'you',\n",
       "  'can',\n",
       "  'use',\n",
       "  'the',\n",
       "  'strip',\n",
       "  'function'],\n",
       " ['Example', 'White', 'spaces', 'removal'],\n",
       " ['Python', 'code'],\n",
       " ['input_str', 't', 'a', 'string', 'examplet', ''],\n",
       " ['input_str', 'input_strstrip'],\n",
       " ['input_str'],\n",
       " ['Output'],\n",
       " ['a', 'string', 'example'],\n",
       " ['Tokenization'],\n",
       " ['Tokenization',\n",
       "  'is',\n",
       "  'the',\n",
       "  'process',\n",
       "  'of',\n",
       "  'splitting',\n",
       "  'the',\n",
       "  'given',\n",
       "  'text',\n",
       "  'into',\n",
       "  'smaller',\n",
       "  'pieces',\n",
       "  'called',\n",
       "  'tokens',\n",
       "  'Words',\n",
       "  'numbers',\n",
       "  'punctuation',\n",
       "  'marks',\n",
       "  'and',\n",
       "  'others',\n",
       "  'can',\n",
       "  'be',\n",
       "  'considered',\n",
       "  'as',\n",
       "  'tokens',\n",
       "  'In',\n",
       "  'this',\n",
       "  'table',\n",
       "  'Tokenization',\n",
       "  'sheet',\n",
       "  'several',\n",
       "  'tools',\n",
       "  'for',\n",
       "  'implementing',\n",
       "  'tokenization',\n",
       "  'are',\n",
       "  'described'],\n",
       " ['Tokenization', 'tools'],\n",
       " ['Remove', 'stop', 'words'],\n",
       " ['Stop',\n",
       "  'words',\n",
       "  'are',\n",
       "  'the',\n",
       "  'most',\n",
       "  'common',\n",
       "  'words',\n",
       "  'in',\n",
       "  'a',\n",
       "  'language',\n",
       "  'like',\n",
       "  'the',\n",
       "  'a',\n",
       "  'on',\n",
       "  'is',\n",
       "  'all',\n",
       "  'These',\n",
       "  'words',\n",
       "  'do',\n",
       "  'not',\n",
       "  'carry',\n",
       "  'important',\n",
       "  'meaning',\n",
       "  'and',\n",
       "  'are',\n",
       "  'usually',\n",
       "  'removed',\n",
       "  'from',\n",
       "  'texts',\n",
       "  'It',\n",
       "  'is',\n",
       "  'possible',\n",
       "  'to',\n",
       "  'remove',\n",
       "  'stop',\n",
       "  'words',\n",
       "  'using',\n",
       "  'Natural',\n",
       "  'Language',\n",
       "  'Toolkit',\n",
       "  'NLTK',\n",
       "  'a',\n",
       "  'suite',\n",
       "  'of',\n",
       "  'libraries',\n",
       "  'and',\n",
       "  'programs',\n",
       "  'for',\n",
       "  'symbolic',\n",
       "  'and',\n",
       "  'statistical',\n",
       "  'natural',\n",
       "  'language',\n",
       "  'processing'],\n",
       " ['Example', 'Stop', 'words', 'removal'],\n",
       " ['Code'],\n",
       " ['input_str',\n",
       "  'NLTK',\n",
       "  'is',\n",
       "  'a',\n",
       "  'leading',\n",
       "  'platform',\n",
       "  'for',\n",
       "  'building',\n",
       "  'Python',\n",
       "  'programs',\n",
       "  'to',\n",
       "  'work',\n",
       "  'with',\n",
       "  'human',\n",
       "  'language',\n",
       "  'data'],\n",
       " ['stop_words', 'setstopwordswordsenglish'],\n",
       " ['from', 'nltktokenize', 'import', 'word_tokenize'],\n",
       " ['tokens', 'word_tokenizeinput_str'],\n",
       " ['result',\n",
       "  'i',\n",
       "  'for',\n",
       "  'i',\n",
       "  'in',\n",
       "  'tokens',\n",
       "  'if',\n",
       "  'not',\n",
       "  'i',\n",
       "  'in',\n",
       "  'stop_words'],\n",
       " ['print', 'result'],\n",
       " ['Output'],\n",
       " ['NLTK',\n",
       "  'leading',\n",
       "  'platform',\n",
       "  'building',\n",
       "  'Python',\n",
       "  'programs',\n",
       "  'work',\n",
       "  'human',\n",
       "  'language',\n",
       "  'data',\n",
       "  ''],\n",
       " ['A',\n",
       "  'scikitlearn',\n",
       "  'tool',\n",
       "  'also',\n",
       "  'provides',\n",
       "  'a',\n",
       "  'stop',\n",
       "  'words',\n",
       "  'list'],\n",
       " ['from',\n",
       "  'sklearnfeature_extractionstop_words',\n",
       "  'import',\n",
       "  'ENGLISH_STOP_WORDS'],\n",
       " ['Its',\n",
       "  'also',\n",
       "  'possible',\n",
       "  'to',\n",
       "  'use',\n",
       "  'spaCy',\n",
       "  'a',\n",
       "  'free',\n",
       "  'opensource',\n",
       "  'library'],\n",
       " ['from', 'spacylangenstop_words', 'import', 'STOP_WORDS'],\n",
       " ['Remove', 'sparse', 'terms', 'and', 'particular', 'words'],\n",
       " ['In',\n",
       "  'some',\n",
       "  'cases',\n",
       "  'its',\n",
       "  'necessary',\n",
       "  'to',\n",
       "  'remove',\n",
       "  'sparse',\n",
       "  'terms',\n",
       "  'or',\n",
       "  'particular',\n",
       "  'words',\n",
       "  'from',\n",
       "  'texts',\n",
       "  'This',\n",
       "  'task',\n",
       "  'can',\n",
       "  'be',\n",
       "  'done',\n",
       "  'using',\n",
       "  'stop',\n",
       "  'words',\n",
       "  'removal',\n",
       "  'techniques',\n",
       "  'considering',\n",
       "  'that',\n",
       "  'any',\n",
       "  'group',\n",
       "  'of',\n",
       "  'words',\n",
       "  'can',\n",
       "  'be',\n",
       "  'chosen',\n",
       "  'as',\n",
       "  'the',\n",
       "  'stop',\n",
       "  'words'],\n",
       " ['Stemming'],\n",
       " ['Stemming',\n",
       "  'is',\n",
       "  'a',\n",
       "  'process',\n",
       "  'of',\n",
       "  'reducing',\n",
       "  'words',\n",
       "  'to',\n",
       "  'their',\n",
       "  'word',\n",
       "  'stem',\n",
       "  'base',\n",
       "  'or',\n",
       "  'root',\n",
       "  'form',\n",
       "  'for',\n",
       "  'example',\n",
       "  'booksbook',\n",
       "  'lookedlook',\n",
       "  'The',\n",
       "  'main',\n",
       "  'two',\n",
       "  'algorithms',\n",
       "  'are',\n",
       "  'Porter',\n",
       "  'stemming',\n",
       "  'algorithm',\n",
       "  'removes',\n",
       "  'common',\n",
       "  'morphological',\n",
       "  'and',\n",
       "  'inflexional',\n",
       "  'endings',\n",
       "  'from',\n",
       "  'words',\n",
       "  'and',\n",
       "  'Lancaster',\n",
       "  'stemming',\n",
       "  'algorithm',\n",
       "  'a',\n",
       "  'more',\n",
       "  'aggressive',\n",
       "  'stemming',\n",
       "  'algorithm',\n",
       "  'In',\n",
       "  'the',\n",
       "  'Stemming',\n",
       "  'sheet',\n",
       "  'of',\n",
       "  'the',\n",
       "  'table',\n",
       "  'some',\n",
       "  'stemmers',\n",
       "  'are',\n",
       "  'described'],\n",
       " ['Stemming', 'tools'],\n",
       " ['Example', 'Stemming', 'using', 'NLTK'],\n",
       " ['Code'],\n",
       " ['from', 'nltkstem', 'import', 'PorterStemmer'],\n",
       " ['from', 'nltktokenize', 'import', 'word_tokenize'],\n",
       " ['stemmer', 'PorterStemmer'],\n",
       " ['input_strThere', 'are', 'several', 'types', 'of', 'stemming', 'algorithms'],\n",
       " ['input_strword_tokenizeinput_str'],\n",
       " ['for', 'word', 'in', 'input_str'],\n",
       " ['', 'printstemmerstemword'],\n",
       " ['Output'],\n",
       " ['There', 'are', 'sever', 'type', 'of', 'stem', 'algorithm'],\n",
       " ['Lemmatization'],\n",
       " ['The',\n",
       "  'aim',\n",
       "  'of',\n",
       "  'lemmatization',\n",
       "  'like',\n",
       "  'stemming',\n",
       "  'is',\n",
       "  'to',\n",
       "  'reduce',\n",
       "  'inflectional',\n",
       "  'forms',\n",
       "  'to',\n",
       "  'a',\n",
       "  'common',\n",
       "  'base',\n",
       "  'form',\n",
       "  'As',\n",
       "  'opposed',\n",
       "  'to',\n",
       "  'stemming',\n",
       "  'lemmatization',\n",
       "  'does',\n",
       "  'not',\n",
       "  'simply',\n",
       "  'chop',\n",
       "  'off',\n",
       "  'inflections',\n",
       "  'Instead',\n",
       "  'it',\n",
       "  'uses',\n",
       "  'lexical',\n",
       "  'knowledge',\n",
       "  'bases',\n",
       "  'to',\n",
       "  'get',\n",
       "  'the',\n",
       "  'correct',\n",
       "  'base',\n",
       "  'forms',\n",
       "  'of',\n",
       "  'words'],\n",
       " ['Lemmatization',\n",
       "  'tools',\n",
       "  'are',\n",
       "  'presented',\n",
       "  'libraries',\n",
       "  'described',\n",
       "  'above',\n",
       "  'NLTK',\n",
       "  'WordNet',\n",
       "  'Lemmatizer',\n",
       "  'spaCy',\n",
       "  'TextBlob',\n",
       "  'Pattern',\n",
       "  'gensim',\n",
       "  'Stanford',\n",
       "  'CoreNLP',\n",
       "  'MemoryBased',\n",
       "  'Shallow',\n",
       "  'Parser',\n",
       "  'MBSP',\n",
       "  'Apache',\n",
       "  'OpenNLP',\n",
       "  'Apache',\n",
       "  'Lucene',\n",
       "  'General',\n",
       "  'Architecture',\n",
       "  'for',\n",
       "  'Text',\n",
       "  'Engineering',\n",
       "  'GATE',\n",
       "  'Illinois',\n",
       "  'Lemmatizer',\n",
       "  'and',\n",
       "  'DKPro',\n",
       "  'Core'],\n",
       " ['Example', 'Lemmatization', 'using', 'NLTK'],\n",
       " ['Code'],\n",
       " ['from', 'nltkstem', 'import', 'WordNetLemmatizer'],\n",
       " ['from', 'nltktokenize', 'import', 'word_tokenize'],\n",
       " ['lemmatizerWordNetLemmatizer'],\n",
       " ['input_strbeen', 'had', 'done', 'languages', 'cities', 'mice'],\n",
       " ['input_strword_tokenizeinput_str'],\n",
       " ['for', 'word', 'in', 'input_str'],\n",
       " ['', 'printlemmatizerlemmatizeword'],\n",
       " ['Output'],\n",
       " ['be', 'have', 'do', 'language', 'city', 'mouse'],\n",
       " ['Part', 'of', 'speech', 'tagging', 'POS'],\n",
       " ['Partofspeech',\n",
       "  'tagging',\n",
       "  'aims',\n",
       "  'to',\n",
       "  'assign',\n",
       "  'parts',\n",
       "  'of',\n",
       "  'speech',\n",
       "  'to',\n",
       "  'each',\n",
       "  'word',\n",
       "  'of',\n",
       "  'a',\n",
       "  'given',\n",
       "  'text',\n",
       "  'such',\n",
       "  'as',\n",
       "  'nouns',\n",
       "  'verbs',\n",
       "  'adjectives',\n",
       "  'and',\n",
       "  'others',\n",
       "  'based',\n",
       "  'on',\n",
       "  'its',\n",
       "  'definition',\n",
       "  'and',\n",
       "  'its',\n",
       "  'context',\n",
       "  'There',\n",
       "  'are',\n",
       "  'many',\n",
       "  'tools',\n",
       "  'containing',\n",
       "  'POS',\n",
       "  'taggers',\n",
       "  'including',\n",
       "  'NLTK',\n",
       "  'spaCy',\n",
       "  'TextBlob',\n",
       "  'Pattern',\n",
       "  'Stanford',\n",
       "  'CoreNLP',\n",
       "  'MemoryBased',\n",
       "  'Shallow',\n",
       "  'Parser',\n",
       "  'MBSP',\n",
       "  'Apache',\n",
       "  'OpenNLP',\n",
       "  'Apache',\n",
       "  'Lucene',\n",
       "  'General',\n",
       "  'Architecture',\n",
       "  'for',\n",
       "  'Text',\n",
       "  'Engineering',\n",
       "  'GATE',\n",
       "  'FreeLing',\n",
       "  'Illinois',\n",
       "  'Part',\n",
       "  'of',\n",
       "  'Speech',\n",
       "  'Tagger',\n",
       "  'and',\n",
       "  'DKPro',\n",
       "  'Core'],\n",
       " ['Example', 'Partofspeech', 'tagging', 'using', 'TextBlob'],\n",
       " ['Code'],\n",
       " ['input_strParts',\n",
       "  'of',\n",
       "  'speech',\n",
       "  'examples',\n",
       "  'an',\n",
       "  'article',\n",
       "  'to',\n",
       "  'write',\n",
       "  'interesting',\n",
       "  'easily',\n",
       "  'and',\n",
       "  'of'],\n",
       " ['from', 'textblob', 'import', 'TextBlob'],\n",
       " ['result', 'TextBlobinput_str'],\n",
       " ['printresulttags'],\n",
       " ['Output'],\n",
       " ['Parts',\n",
       "  'uNNS',\n",
       "  'of',\n",
       "  'uIN',\n",
       "  'speech',\n",
       "  'uNN',\n",
       "  'examples',\n",
       "  'uNNS',\n",
       "  'an',\n",
       "  'uDT',\n",
       "  'article',\n",
       "  'uNN',\n",
       "  'to',\n",
       "  'uTO',\n",
       "  'write',\n",
       "  'uVB',\n",
       "  'interesting',\n",
       "  'uVBG',\n",
       "  'easily',\n",
       "  'uRB',\n",
       "  'and',\n",
       "  'uCC',\n",
       "  'of',\n",
       "  'uIN'],\n",
       " ['Chunking', 'shallow', 'parsing'],\n",
       " ['Chunking',\n",
       "  'is',\n",
       "  'a',\n",
       "  'natural',\n",
       "  'language',\n",
       "  'process',\n",
       "  'that',\n",
       "  'identifies',\n",
       "  'constituent',\n",
       "  'parts',\n",
       "  'of',\n",
       "  'sentences',\n",
       "  'nouns',\n",
       "  'verbs',\n",
       "  'adjectives',\n",
       "  'etc',\n",
       "  'and',\n",
       "  'links',\n",
       "  'them',\n",
       "  'to',\n",
       "  'higher',\n",
       "  'order',\n",
       "  'units',\n",
       "  'that',\n",
       "  'have',\n",
       "  'discrete',\n",
       "  'grammatical',\n",
       "  'meanings',\n",
       "  'noun',\n",
       "  'groups',\n",
       "  'or',\n",
       "  'phrases',\n",
       "  'verb',\n",
       "  'groups',\n",
       "  'etc',\n",
       "  'Chunking',\n",
       "  'tools',\n",
       "  'NLTK',\n",
       "  'TreeTagger',\n",
       "  'chunker',\n",
       "  'Apache',\n",
       "  'OpenNLP',\n",
       "  'General',\n",
       "  'Architecture',\n",
       "  'for',\n",
       "  'Text',\n",
       "  'Engineering',\n",
       "  'GATE',\n",
       "  'FreeLing'],\n",
       " ['Example', 'Chunking', 'using', 'NLTK'],\n",
       " ['The',\n",
       "  'first',\n",
       "  'step',\n",
       "  'is',\n",
       "  'to',\n",
       "  'determine',\n",
       "  'the',\n",
       "  'part',\n",
       "  'of',\n",
       "  'speech',\n",
       "  'for',\n",
       "  'each',\n",
       "  'word'],\n",
       " ['Code'],\n",
       " ['input_strA',\n",
       "  'black',\n",
       "  'television',\n",
       "  'and',\n",
       "  'a',\n",
       "  'white',\n",
       "  'stove',\n",
       "  'were',\n",
       "  'bought',\n",
       "  'for',\n",
       "  'the',\n",
       "  'new',\n",
       "  'apartment',\n",
       "  'of',\n",
       "  'John'],\n",
       " ['from', 'textblob', 'import', 'TextBlob'],\n",
       " ['result', 'TextBlobinput_str'],\n",
       " ['printresulttags'],\n",
       " ['Output'],\n",
       " ['A',\n",
       "  'uDT',\n",
       "  'black',\n",
       "  'uJJ',\n",
       "  'television',\n",
       "  'uNN',\n",
       "  'and',\n",
       "  'uCC',\n",
       "  'a',\n",
       "  'uDT',\n",
       "  'white',\n",
       "  'uJJ',\n",
       "  'stove',\n",
       "  'uNN',\n",
       "  'were',\n",
       "  'uVBD',\n",
       "  'bought',\n",
       "  'uVBN',\n",
       "  'for',\n",
       "  'uIN',\n",
       "  'the',\n",
       "  'uDT',\n",
       "  'new',\n",
       "  'uJJ',\n",
       "  'apartment',\n",
       "  'uNN',\n",
       "  'of',\n",
       "  'uIN',\n",
       "  'John',\n",
       "  'uNNP'],\n",
       " ['The', 'second', 'step', 'is', 'chunking'],\n",
       " ['Code'],\n",
       " ['reg_exp', 'NP', 'DTJJNN'],\n",
       " ['rp', 'nltkRegexpParserreg_exp'],\n",
       " ['result', 'rpparseresulttags'],\n",
       " ['printresult'],\n",
       " ['Output'],\n",
       " ['S',\n",
       "  'NP',\n",
       "  'ADT',\n",
       "  'blackJJ',\n",
       "  'televisionNN',\n",
       "  'andCC',\n",
       "  'NP',\n",
       "  'aDT',\n",
       "  'whiteJJ',\n",
       "  'stoveNN',\n",
       "  'wereVBD',\n",
       "  'boughtVBN',\n",
       "  'forIN',\n",
       "  'NP',\n",
       "  'theDT',\n",
       "  'newJJ',\n",
       "  'apartmentNN'],\n",
       " ['ofIN', 'JohnNNP'],\n",
       " ['Its',\n",
       "  'also',\n",
       "  'possible',\n",
       "  'to',\n",
       "  'draw',\n",
       "  'the',\n",
       "  'sentence',\n",
       "  'tree',\n",
       "  'structure',\n",
       "  'using',\n",
       "  'code',\n",
       "  'resultdraw'],\n",
       " ['Named', 'entity', 'recognition'],\n",
       " ['Namedentity',\n",
       "  'recognition',\n",
       "  'NER',\n",
       "  'aims',\n",
       "  'to',\n",
       "  'find',\n",
       "  'named',\n",
       "  'entities',\n",
       "  'in',\n",
       "  'text',\n",
       "  'and',\n",
       "  'classify',\n",
       "  'them',\n",
       "  'into',\n",
       "  'predefined',\n",
       "  'categories',\n",
       "  'names',\n",
       "  'of',\n",
       "  'persons',\n",
       "  'locations',\n",
       "  'organizations',\n",
       "  'times',\n",
       "  'etc'],\n",
       " ['Namedentity',\n",
       "  'recognition',\n",
       "  'tools',\n",
       "  'NLTK',\n",
       "  'spaCy',\n",
       "  'General',\n",
       "  'Architecture',\n",
       "  'for',\n",
       "  'Text',\n",
       "  'Engineering',\n",
       "  'GATEANNIE',\n",
       "  'Apache',\n",
       "  'OpenNLP',\n",
       "  'Stanford',\n",
       "  'CoreNLP',\n",
       "  'DKPro',\n",
       "  'Core',\n",
       "  'MITIE',\n",
       "  'Watson',\n",
       "  'Natural',\n",
       "  'Language',\n",
       "  'Understanding',\n",
       "  'TextRazor',\n",
       "  'FreeLing',\n",
       "  'are',\n",
       "  'described',\n",
       "  'in',\n",
       "  'the',\n",
       "  'NER',\n",
       "  'sheet',\n",
       "  'of',\n",
       "  'the',\n",
       "  'table'],\n",
       " ['NER', 'Tools'],\n",
       " ['Example', 'Namedentity', 'recognition', 'using', 'NLTK'],\n",
       " ['Code'],\n",
       " ['from', 'nltk', 'import', 'word_tokenize', 'pos_tag', 'ne_chunk'],\n",
       " ['input_str',\n",
       "  'Bill',\n",
       "  'works',\n",
       "  'for',\n",
       "  'Apple',\n",
       "  'so',\n",
       "  'he',\n",
       "  'went',\n",
       "  'to',\n",
       "  'Boston',\n",
       "  'for',\n",
       "  'a',\n",
       "  'conference'],\n",
       " ['print', 'ne_chunkpos_tagword_tokenizeinput_str'],\n",
       " ['Output'],\n",
       " ['S',\n",
       "  'PERSON',\n",
       "  'BillNNP',\n",
       "  'worksVBZ',\n",
       "  'forIN',\n",
       "  'AppleNNP',\n",
       "  'soIN',\n",
       "  'hePRP',\n",
       "  'wentVBD',\n",
       "  'toTO',\n",
       "  'GPE',\n",
       "  'BostonNNP',\n",
       "  'forIN',\n",
       "  'aDT',\n",
       "  'conferenceNN',\n",
       "  ''],\n",
       " ['Coreference', 'resolution', 'anaphora', 'resolution'],\n",
       " ['Pronouns',\n",
       "  'and',\n",
       "  'other',\n",
       "  'referring',\n",
       "  'expressions',\n",
       "  'should',\n",
       "  'be',\n",
       "  'connected',\n",
       "  'to',\n",
       "  'the',\n",
       "  'right',\n",
       "  'individuals',\n",
       "  'Coreference',\n",
       "  'resolution',\n",
       "  'finds',\n",
       "  'the',\n",
       "  'mentions',\n",
       "  'in',\n",
       "  'a',\n",
       "  'text',\n",
       "  'that',\n",
       "  'refer',\n",
       "  'to',\n",
       "  'the',\n",
       "  'same',\n",
       "  'realworld',\n",
       "  'entity',\n",
       "  'For',\n",
       "  'example',\n",
       "  'in',\n",
       "  'the',\n",
       "  'sentence',\n",
       "  'Andrew',\n",
       "  'said',\n",
       "  'he',\n",
       "  'would',\n",
       "  'buy',\n",
       "  'a',\n",
       "  'car',\n",
       "  'the',\n",
       "  'pronoun',\n",
       "  'he',\n",
       "  'refers',\n",
       "  'to',\n",
       "  'the',\n",
       "  'same',\n",
       "  'person',\n",
       "  'namely',\n",
       "  'to',\n",
       "  'Andrew',\n",
       "  'Coreference',\n",
       "  'resolution',\n",
       "  'tools',\n",
       "  'Stanford',\n",
       "  'CoreNLP',\n",
       "  'spaCy',\n",
       "  'Open',\n",
       "  'Calais',\n",
       "  'Apache',\n",
       "  'OpenNLP',\n",
       "  'are',\n",
       "  'described',\n",
       "  'in',\n",
       "  'the',\n",
       "  'Coreference',\n",
       "  'resolution',\n",
       "  'sheet',\n",
       "  'of',\n",
       "  'the',\n",
       "  'table'],\n",
       " ['Coreference', 'resolution', 'tools'],\n",
       " ['An',\n",
       "  'example',\n",
       "  'of',\n",
       "  'coreference',\n",
       "  'resolution',\n",
       "  'using',\n",
       "  'xrenner',\n",
       "  'can',\n",
       "  'be',\n",
       "  'found',\n",
       "  'here'],\n",
       " ['Collocation', 'extraction'],\n",
       " ['Collocations',\n",
       "  'are',\n",
       "  'word',\n",
       "  'combinations',\n",
       "  'occurring',\n",
       "  'together',\n",
       "  'more',\n",
       "  'often',\n",
       "  'than',\n",
       "  'would',\n",
       "  'be',\n",
       "  'expected',\n",
       "  'by',\n",
       "  'chance',\n",
       "  'Collocation',\n",
       "  'examples',\n",
       "  'are',\n",
       "  'break',\n",
       "  'the',\n",
       "  'rules',\n",
       "  'free',\n",
       "  'time',\n",
       "  'draw',\n",
       "  'a',\n",
       "  'conclusion',\n",
       "  'keep',\n",
       "  'in',\n",
       "  'mind',\n",
       "  'get',\n",
       "  'ready',\n",
       "  'and',\n",
       "  'so',\n",
       "  'on'],\n",
       " ['Collocation', 'extraction', 'tools'],\n",
       " ['Example', 'Collocation', 'extraction', 'using', 'ICE', ''],\n",
       " ['Code'],\n",
       " ['inputhe',\n",
       "  'and',\n",
       "  'Chazz',\n",
       "  'duel',\n",
       "  'with',\n",
       "  'all',\n",
       "  'keys',\n",
       "  'on',\n",
       "  'the',\n",
       "  'line'],\n",
       " ['from', 'ICE', 'import', 'CollocationExtractor'],\n",
       " ['extractor',\n",
       "  'CollocationExtractorwith_collocation_pipelineT',\n",
       "  'bing_key',\n",
       "  'Temppos_check',\n",
       "  'False'],\n",
       " ['printextractorget_collocations_of_lengthinput', 'length', ''],\n",
       " ['Output'],\n",
       " ['on', 'the', 'line'],\n",
       " ['Relationship', 'extraction'],\n",
       " ['Relationship',\n",
       "  'extraction',\n",
       "  'allows',\n",
       "  'obtaining',\n",
       "  'structured',\n",
       "  'information',\n",
       "  'from',\n",
       "  'unstructured',\n",
       "  'sources',\n",
       "  'such',\n",
       "  'as',\n",
       "  'raw',\n",
       "  'text',\n",
       "  'Strictly',\n",
       "  'stated',\n",
       "  'it',\n",
       "  'is',\n",
       "  'identifying',\n",
       "  'relations',\n",
       "  'eg',\n",
       "  'acquisition',\n",
       "  'spouse',\n",
       "  'employment',\n",
       "  'among',\n",
       "  'named',\n",
       "  'entities',\n",
       "  'eg',\n",
       "  'people',\n",
       "  'organizations',\n",
       "  'locations',\n",
       "  'For',\n",
       "  'example',\n",
       "  'from',\n",
       "  'the',\n",
       "  'sentence',\n",
       "  'Mark',\n",
       "  'and',\n",
       "  'Emily',\n",
       "  'married',\n",
       "  'yesterday',\n",
       "  'we',\n",
       "  'can',\n",
       "  'extract',\n",
       "  'the',\n",
       "  'information',\n",
       "  'that',\n",
       "  'Mark',\n",
       "  'is',\n",
       "  'Emilys',\n",
       "  'husband'],\n",
       " ['An',\n",
       "  'example',\n",
       "  'of',\n",
       "  'relationship',\n",
       "  'extraction',\n",
       "  'using',\n",
       "  'NLTK',\n",
       "  'can',\n",
       "  'be',\n",
       "  'found',\n",
       "  'here'],\n",
       " ['Summary'],\n",
       " ['In',\n",
       "  'this',\n",
       "  'post',\n",
       "  'we',\n",
       "  'talked',\n",
       "  'about',\n",
       "  'text',\n",
       "  'preprocessing',\n",
       "  'and',\n",
       "  'described',\n",
       "  'its',\n",
       "  'main',\n",
       "  'steps',\n",
       "  'including',\n",
       "  'normalization',\n",
       "  'tokenization',\n",
       "  'stemming',\n",
       "  'lemmatization',\n",
       "  'chunking',\n",
       "  'part',\n",
       "  'of',\n",
       "  'speech',\n",
       "  'tagging',\n",
       "  'namedentity',\n",
       "  'recognition',\n",
       "  'coreference',\n",
       "  'resolution',\n",
       "  'collocation',\n",
       "  'extraction',\n",
       "  'and',\n",
       "  'relationship',\n",
       "  'extraction',\n",
       "  'We',\n",
       "  'also',\n",
       "  'discussed',\n",
       "  'text',\n",
       "  'preprocessing',\n",
       "  'tools',\n",
       "  'and',\n",
       "  'examples',\n",
       "  'A',\n",
       "  'comparative',\n",
       "  'table',\n",
       "  'was',\n",
       "  'created'],\n",
       " ['After',\n",
       "  'the',\n",
       "  'text',\n",
       "  'preprocessing',\n",
       "  'is',\n",
       "  'done',\n",
       "  'the',\n",
       "  'result',\n",
       "  'may',\n",
       "  'be',\n",
       "  'used',\n",
       "  'for',\n",
       "  'more',\n",
       "  'complicated',\n",
       "  'NLP',\n",
       "  'tasks',\n",
       "  'for',\n",
       "  'example',\n",
       "  'machine',\n",
       "  'translation',\n",
       "  'or',\n",
       "  'natural',\n",
       "  'language',\n",
       "  'generation']]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp.each_word_count"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
