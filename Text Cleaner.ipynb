{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# v 0.3.1\n",
    "import regex as re,string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords \n",
    "stop_words = set(stopwords.words('english'))\n",
    "from nltk.stem import SnowballStemmer\n",
    "stemmer= SnowballStemmer('english')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "import os.path\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#removes all the blank line from the text file\n",
    "#returns list\n",
    "def clear_blank_lines(file):\n",
    "    return document(file).clear_blank_lines().data\n",
    "\n",
    "# it removes \".\\n\" from every element by default\n",
    "# can be used to strip by second argument\n",
    "def strip_all(file,x='.\\n'):\n",
    "    return document(file).strip_all(x).data\n",
    "\n",
    "\n",
    "# converts each character to lowercase\n",
    "def lower_all(file):\n",
    "    return document(file).lower_all().data\n",
    "\n",
    "# removes numbers detected anywhere in the data\n",
    "def remove_numbers(file):\n",
    "    return document(file).remove_numbers().data\n",
    "\n",
    "# removes punctuations detected anywhere in the data\n",
    "def remove_symbols(file):\n",
    "    return document(file).remove_symbols().data\n",
    "\n",
    "# it will remove stop words and return a list of list of words  \n",
    "def remove_stpwrds(file,op='sents'):\n",
    "    return document(file).remove_stpwrds().data\n",
    "\n",
    "#for tokenization\n",
    "def token_it(file):\n",
    "    return document(file).token_it()\n",
    "\n",
    "# reduces each word to its stem work like, dogs to dog\n",
    "def stemming(file):\n",
    "    return document(file).stemming().data\n",
    "    \n",
    "# gets the root word for each word\n",
    "def lemming(file):\n",
    "    return document(file).lemming().data\n",
    "    \n",
    "\n",
    "def main_cleaner(file,op = 'sents'):\n",
    "    return document(file).main_cleaner().data\n",
    "\n",
    "def formating(block):\n",
    "    return [' '.join(each) for each in block]\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class document:\n",
    "    file_name = ''\n",
    "    data = []\n",
    "    words = []\n",
    "    tokens = []\n",
    "    total_sentences = 0\n",
    "    total_words = 0\n",
    "    each_word_count = {}\n",
    "    \n",
    "    def __init__(self,fname=''):\n",
    "        \n",
    "        if fname:            \n",
    "            self.data,fame =  self._reader(fname)\n",
    "            self.words = [each.split() for each in self.data if type(each)]\n",
    "            self.total_words = sum([len(each.split(' ')) for each in self.data])\n",
    "            a = [word for sent in [word_tokenize(each) for each in self.data] for word in sent]        \n",
    "            self.each_word_count={x:a.count(x) for x in a}\n",
    "            self.total_sentences = self.each_word_count['.']\n",
    "            self.file_name = fame.split('\\\\')[-1]\n",
    "        else:\n",
    "            pass    \n",
    "        \n",
    "    def __repr__(self):\n",
    "         return '\\n'.join(map(str, self.data))\n",
    "\n",
    "    def __str__(self):\n",
    "        return '\\n'.join(map(str, self.data))\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for i in self.data: yield i\n",
    "    \n",
    "    def copy(self):\n",
    "        temp = copy.deepcopy(self)       \n",
    "        return temp\n",
    "\n",
    "    # checks whether it has file path as argument\n",
    "    def _file_or_not(self,arg):\n",
    "        if os.path.isfile(arg):\n",
    "            return True\n",
    "        else:\n",
    "            return False,\"only supports .txt for now\"\n",
    "\n",
    "    # this reader is flexible enough to process file or will return the data if list is being passed to the function.\n",
    "    def _reader(self,file):\n",
    "        if type(file)==str and self._file_or_not(file)==True:\n",
    "            with open(file,'r') as f:\n",
    "                return f.readlines(),file\n",
    "\n",
    "        elif type(file)==str:\n",
    "            return file.split('\\n'),''\n",
    "\n",
    "        else: return file,''\n",
    "\n",
    "\n",
    "    \n",
    "    #removes all the blank line from the text file\n",
    "    #returns list\n",
    "    def clear_blank_lines(self,inplace=False):\n",
    "        if not inplace: self = self.copy()\n",
    "        self.data =  list(filter(str.strip,[each.rstrip() for each in self.data]))\n",
    "        return self\n",
    "\n",
    "    \n",
    "    # it removes \".\\n\" from every element by default\n",
    "    # can be used to strip by second argument\n",
    "    def strip_all(self,x='.\\n',inplace=False):\n",
    "        if not inplace: self = self.copy()\n",
    "        self.data = [re.sub(r'\\s+',' ',each.strip(x)) for each in self.data]\n",
    "        return self\n",
    "\n",
    "\n",
    "    # converts each character to lowercase\n",
    "    def lower_all(self,inplace=False):\n",
    "        if not inplace: self = self.copy()\n",
    "        self.data = [each.lower() for each in self.data]\n",
    "        return self\n",
    "\n",
    "    # removes numbers detected anywhere in the data\n",
    "    def remove_numbers(self,inplace=False):\n",
    "        if not inplace: self = self.copy()\n",
    "        self.data = [re.sub(r'[0-9]+', '',(each)) for each in self.data]\n",
    "        return self\n",
    "\n",
    "    # removes punctuations detected anywhere in the data\n",
    "    def remove_symbols(self,inplace=False):\n",
    "        if not inplace: self = self.copy()\n",
    "        self.data = [re.sub(r'[^\\w\\s]','',each) for each in self.data]\n",
    "        return self.strip_all()\n",
    "\n",
    "    \n",
    "    # it will remove stop words and return a list of list of words  \n",
    "    def remove_stpwrds(self,inplace=False):\n",
    "        if not inplace: self = self.copy()\n",
    "        self.words = [[w for w in each.split() if not w in stop_words] for each in self.data] \n",
    "    \n",
    "        self.data = formating(self.words)        \n",
    "        return self\n",
    "        \n",
    "\n",
    "    #for tokenization this function can't be use as object \n",
    "    def token_it(self):\n",
    "        self.tokens = [word_tokenize(each) for each in self.data]\n",
    "        return self.tokens\n",
    "\n",
    "    # reduces each word to its stem work like, dogs to dog\n",
    "    def stemming(self,inplace=False):\n",
    "        if not inplace: self = self.copy()\n",
    "        self.data = formating([[stemmer.stem(word) for word in each.split()] for each in self.data])\n",
    "        return self\n",
    "\n",
    "    # gets the root word for each word\n",
    "    def lemming(self,inplace=False):\n",
    "        if not inplace: self = self.copy()\n",
    "        self.data = formating([[lemmatizer.lemmatize(word) for word in each.split()] for each in self.data])\n",
    "        return self\n",
    "\n",
    "\n",
    "    def main_cleaner(self,op = 'sents',inplace=False):\n",
    "        if not inplace: self = self.copy()\n",
    "            \n",
    "        # this is the basic cleaning which operates with each line\n",
    "        part1 = self.clear_blank_lines().strip_all().lower_all().remove_numbers().remove_symbols()\n",
    "\n",
    "        # this is the advanced cleaning which operates with each word\n",
    "        part2 = part1.lemming().remove_stpwrds()\n",
    "\n",
    "        if op== 'sents':\n",
    "            return part2\n",
    "\n",
    "        if op== 'words':\n",
    "            return [word for sent in part2.data for word in sent.split()]        \n",
    "\n",
    "        if op not in ('sents','words'):\n",
    "            return \"value of option is not valid, try 'sents' or 'words' instead\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document('file.txt').tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
