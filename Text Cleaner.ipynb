{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# v 0.3.1\n",
    "import regex as re,string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords \n",
    "stop_words = set(stopwords.words('english'))\n",
    "from nltk.stem import SnowballStemmer\n",
    "stemmer= SnowballStemmer('english')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "import os.path\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#removes all the blank line from the text file\n",
    "#returns list\n",
    "def clear_blank_lines(file):\n",
    "    return document(file).clear_blank_lines().data\n",
    "\n",
    "# it removes \".\\n\" from every element by default\n",
    "# can be used to strip by second argument\n",
    "def strip_all(file,x='.\\n'):\n",
    "    return document(file).strip_all(x).data\n",
    "\n",
    "\n",
    "# converts each character to lowercase\n",
    "def lower_all(file):\n",
    "    return document(file).lower_all().data\n",
    "\n",
    "# removes numbers detected anywhere in the data\n",
    "def remove_numbers(file):\n",
    "    return document(file).remove_numbers().data\n",
    "\n",
    "# removes punctuations detected anywhere in the data\n",
    "def remove_symbols(file):\n",
    "    return document(file).remove_symbols().data\n",
    "\n",
    "# it will remove stop words and return a list of list of words  \n",
    "def remove_stpwrds(file,op='sents'):\n",
    "    return document(file).remove_stpwrds().data\n",
    "\n",
    "#for tokenization\n",
    "def token_it(file):\n",
    "    return document(file).token_it()\n",
    "\n",
    "# reduces each word to its stem work like, dogs to dog\n",
    "def stemming(file):\n",
    "    return document(file).stemming().data\n",
    "    \n",
    "# gets the root word for each word\n",
    "def lemming(file):\n",
    "    return document(file).lemming().data\n",
    "    \n",
    "\n",
    "def main_cleaner(file,op = 'sents'):\n",
    "    return document(file).main_cleaner().data\n",
    "\n",
    "def formating(block):\n",
    "    return [' '.join(each) for each in block]\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class document:\n",
    "    file_name = ''\n",
    "    data = []\n",
    "    words = []\n",
    "    tokens = []\n",
    "    total_sentences = 0\n",
    "    total_words = 0\n",
    "    each_word_count = {}\n",
    "    \n",
    "    def __init__(self,fname=''):\n",
    "        \n",
    "        if fname:            \n",
    "            self.data,fame =  self._reader(fname)\n",
    "            self.words = [each.split() for each in self.data if type(each)]\n",
    "            self.total_words = sum([len(each.split(' ')) for each in self.data])\n",
    "            a = [word for sent in [word_tokenize(each) for each in self.data] for word in sent]        \n",
    "            self.each_word_count={x:a.count(x) for x in a}\n",
    "            self.total_sentences = self.each_word_count['.']\n",
    "            self.file_name = fame.split('\\\\')[-1]\n",
    "        else:\n",
    "            pass    \n",
    "        \n",
    "    def __repr__(self):\n",
    "         return '\\n'.join(map(str, self.data))\n",
    "\n",
    "    def __str__(self):\n",
    "        return '\\n'.join(map(str, self.data))\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for i in self.data: yield i\n",
    "    \n",
    "    def copy(self):\n",
    "        temp = copy.deepcopy(self)       \n",
    "        return temp\n",
    "\n",
    "    # checks whether it has file path as argument\n",
    "    def _file_or_not(self,arg):\n",
    "        if os.path.isfile(arg):\n",
    "            return True\n",
    "        else:\n",
    "            return False,\"only supports .txt for now\"\n",
    "\n",
    "    # this reader is flexible enough to process file or will return the data if list is being passed to the function.\n",
    "    def _reader(self,file):\n",
    "        if type(file)==str and self._file_or_not(file)==True:\n",
    "            with open(file,'r') as f:\n",
    "                return f.readlines(),file\n",
    "\n",
    "        elif type(file)==str:\n",
    "            return file.split('\\n'),''\n",
    "\n",
    "        else: return file,''\n",
    "\n",
    "\n",
    "    \n",
    "    #removes all the blank line from the text file\n",
    "    #returns list\n",
    "    def clear_blank_lines(self,inplace=False):\n",
    "        if not inplace: self = self.copy()\n",
    "        self.data =  list(filter(str.strip,[each.rstrip() for each in self.data]))\n",
    "        return self\n",
    "\n",
    "    \n",
    "    # it removes \".\\n\" from every element by default\n",
    "    # can be used to strip by second argument\n",
    "    def strip_all(self,x='.\\n',inplace=False):\n",
    "        if not inplace: self = self.copy()\n",
    "        self.data = [re.sub(r'\\s+',' ',each.strip(x)) for each in self.data]\n",
    "        return self\n",
    "\n",
    "\n",
    "    # converts each character to lowercase\n",
    "    def lower_all(self,inplace=False):\n",
    "        if not inplace: self = self.copy()\n",
    "        self.data = [each.lower() for each in self.data]\n",
    "        return self\n",
    "\n",
    "    # removes numbers detected anywhere in the data\n",
    "    def remove_numbers(self,inplace=False):\n",
    "        if not inplace: self = self.copy()\n",
    "        self.data = [re.sub(r'[0-9]+', '',(each)) for each in self.data]\n",
    "        return self\n",
    "\n",
    "    # removes punctuations detected anywhere in the data\n",
    "    def remove_symbols(self,inplace=False):\n",
    "        if not inplace: self = self.copy()\n",
    "        self.data = [re.sub(r'[^\\w\\s]','',each) for each in self.data]\n",
    "        return self.strip_all()\n",
    "\n",
    "    \n",
    "    # it will remove stop words and return a list of list of words  \n",
    "    def remove_stpwrds(self,inplace=False):\n",
    "        if not inplace: self = self.copy()\n",
    "        self.words = [[w for w in each.split() if not w in stop_words] for each in self.data] \n",
    "    \n",
    "        self.data = formating(self.words)        \n",
    "        return self\n",
    "        \n",
    "\n",
    "    #for tokenization this function can't be use as object \n",
    "    def token_it(self):\n",
    "        self.tokens = [word_tokenize(each) for each in self.data]\n",
    "        return self.tokens\n",
    "\n",
    "    # reduces each word to its stem work like, dogs to dog\n",
    "    def stemming(self,inplace=False):\n",
    "        if not inplace: self = self.copy()\n",
    "        self.data = formating([[stemmer.stem(word) for word in each.split()] for each in self.data])\n",
    "        return self\n",
    "\n",
    "    # gets the root word for each word\n",
    "    def lemming(self,inplace=False):\n",
    "        if not inplace: self = self.copy()\n",
    "        self.data = formating([[lemmatizer.lemmatize(word) for word in each.split()] for each in self.data])\n",
    "        return self\n",
    "\n",
    "\n",
    "    def main_cleaner(self,op = 'sents',inplace=False):\n",
    "        if not inplace: self = self.copy()\n",
    "            \n",
    "        # this is the basic cleaning which operates with each line\n",
    "        part1 = self.clear_blank_lines().strip_all().lower_all().remove_numbers().remove_symbols()\n",
    "\n",
    "        # this is the advanced cleaning which operates with each word\n",
    "        part2 = part1.lemming().remove_stpwrds()\n",
    "\n",
    "        if op== 'sents':\n",
    "            return part2\n",
    "\n",
    "        if op== 'words':\n",
    "            return [word for sent in part2.data for word in sent.split()]        \n",
    "\n",
    "        if op not in ('sents','words'):\n",
    "            return \"value of option is not valid, try 'sents' or 'words' instead\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'!': 5,\n",
       " '#': 3,\n",
       " '$': 2,\n",
       " '%': 1,\n",
       " '&': 2,\n",
       " '(': 92,\n",
       " ')': 92,\n",
       " '*': 1,\n",
       " '*+': 1,\n",
       " ',': 190,\n",
       " '-./': 1,\n",
       " '.': 70,\n",
       " './': 1,\n",
       " '1': 1,\n",
       " '10': 1,\n",
       " '11': 1,\n",
       " '12': 1,\n",
       " '13': 1,\n",
       " '14': 1,\n",
       " '2': 2,\n",
       " '2017': 2,\n",
       " '23': 1,\n",
       " '3': 3,\n",
       " '324': 1,\n",
       " '4': 2,\n",
       " '5': 3,\n",
       " '51': 1,\n",
       " '7': 1,\n",
       " '8': 1,\n",
       " '9': 1,\n",
       " ':': 45,\n",
       " ';': 1,\n",
       " '<': 4,\n",
       " '=': 23,\n",
       " '>': 4,\n",
       " '?': 10,\n",
       " '@': 1,\n",
       " 'A': 6,\n",
       " 'A/DT': 1,\n",
       " 'ANNIE': 1,\n",
       " 'After': 2,\n",
       " 'An': 2,\n",
       " 'Andrew': 2,\n",
       " 'Apache': 7,\n",
       " 'Apple': 1,\n",
       " 'Apple/NNP': 1,\n",
       " 'Architecture': 4,\n",
       " 'As': 1,\n",
       " 'B': 2,\n",
       " 'Bill': 1,\n",
       " 'Bill/NNP': 1,\n",
       " 'Boston': 1,\n",
       " 'Boston/NNP': 1,\n",
       " 'Box': 4,\n",
       " 'Brazil': 1,\n",
       " 'CC': 2,\n",
       " 'Calais': 1,\n",
       " 'Chazz': 1,\n",
       " 'China': 1,\n",
       " 'Chunking': 4,\n",
       " 'Code': 8,\n",
       " 'Collocation': 4,\n",
       " 'CollocationExtractor': 1,\n",
       " 'CollocationExtractor.with_collocation_pipeline': 1,\n",
       " 'Collocations': 1,\n",
       " 'Convert': 2,\n",
       " 'Core': 3,\n",
       " 'CoreNLP': 4,\n",
       " 'Coreference': 5,\n",
       " 'DKPro': 3,\n",
       " 'DT': 5,\n",
       " 'ENGLISH_STOP_WORDS': 1,\n",
       " 'Emily': 2,\n",
       " 'Engineering': 4,\n",
       " 'Example': 11,\n",
       " 'False': 1,\n",
       " 'For': 2,\n",
       " 'FreeLing': 3,\n",
       " 'GATE': 4,\n",
       " 'GPE': 1,\n",
       " 'General': 4,\n",
       " 'ICE': 2,\n",
       " 'IN': 4,\n",
       " 'Illinois': 2,\n",
       " 'In': 5,\n",
       " 'India': 1,\n",
       " 'Indonesia': 1,\n",
       " 'Instead': 1,\n",
       " 'It': 3,\n",
       " 'JJ': 4,\n",
       " 'John': 2,\n",
       " 'John/NNP': 1,\n",
       " 'Lancaster': 1,\n",
       " 'Language': 2,\n",
       " 'Lemmatization': 3,\n",
       " 'Lemmatizer': 2,\n",
       " 'Lucene': 2,\n",
       " 'MBSP': 2,\n",
       " 'MITIE': 1,\n",
       " 'Mark': 2,\n",
       " 'Memory-Based': 2,\n",
       " 'NER': 3,\n",
       " 'NLP': 1,\n",
       " 'NLTK': 12,\n",
       " 'NN': 6,\n",
       " 'NNP': 1,\n",
       " 'NNS': 2,\n",
       " 'NP': 4,\n",
       " 'Named': 1,\n",
       " 'Named-entity': 3,\n",
       " 'Natural': 2,\n",
       " 'Numbers': 1,\n",
       " 'Open': 1,\n",
       " 'OpenNLP': 5,\n",
       " 'Output': 12,\n",
       " 'PERSON': 1,\n",
       " 'POS': 2,\n",
       " 'Parser': 2,\n",
       " 'Part': 2,\n",
       " 'Part-of-speech': 2,\n",
       " 'Parts': 2,\n",
       " 'Pattern': 2,\n",
       " 'Porter': 1,\n",
       " 'PorterStemmer': 2,\n",
       " 'Pronouns': 1,\n",
       " 'Punctuation': 1,\n",
       " 'Python': 6,\n",
       " 'RB': 1,\n",
       " 'Relationship': 2,\n",
       " 'Remove': 6,\n",
       " 'S': 2,\n",
       " 'STOP_WORDS': 1,\n",
       " 'Sample': 1,\n",
       " 'Shallow': 2,\n",
       " 'Speech': 1,\n",
       " 'Stanford': 4,\n",
       " 'States': 1,\n",
       " 'Stemming': 5,\n",
       " 'Stop': 2,\n",
       " 'Strictly': 1,\n",
       " 'Summary': 1,\n",
       " 'T1': 1,\n",
       " 'TO': 1,\n",
       " 'Tagger': 1,\n",
       " 'Temp': 1,\n",
       " 'Text': 5,\n",
       " 'TextBlob': 7,\n",
       " 'TextRazor': 1,\n",
       " 'The': 6,\n",
       " 'There': 3,\n",
       " 'These': 2,\n",
       " 'This': 3,\n",
       " 'To': 1,\n",
       " 'Tokenization': 4,\n",
       " 'Toolkit': 1,\n",
       " 'Tools': 1,\n",
       " 'TreeTagger': 1,\n",
       " 'Understanding': 1,\n",
       " 'United': 1,\n",
       " 'Usually': 1,\n",
       " 'VB': 1,\n",
       " 'VBD': 1,\n",
       " 'VBG': 1,\n",
       " 'VBN': 1,\n",
       " 'Watson': 1,\n",
       " 'We': 3,\n",
       " 'White': 1,\n",
       " 'WordNet': 1,\n",
       " 'WordNetLemmatizer': 1,\n",
       " 'Words': 1,\n",
       " '[': 12,\n",
       " '\\\\': 1,\n",
       " '\\\\d+': 1,\n",
       " '\\\\t': 1,\n",
       " ']': 12,\n",
       " '^_`': 1,\n",
       " 'a': 20,\n",
       " 'a/DT': 2,\n",
       " 'abbreviations': 1,\n",
       " 'about': 2,\n",
       " 'above': 1,\n",
       " 'accent': 1,\n",
       " 'acquisition': 1,\n",
       " 'adjectives': 2,\n",
       " 'aggressive': 1,\n",
       " 'aim': 1,\n",
       " 'aims': 2,\n",
       " 'algorithm': 4,\n",
       " 'algorithms': 2,\n",
       " 'all': 3,\n",
       " 'allows': 1,\n",
       " 'also': 5,\n",
       " 'among': 1,\n",
       " 'an': 4,\n",
       " 'analyses': 1,\n",
       " 'anaphora': 1,\n",
       " 'and': 33,\n",
       " 'and/CC': 1,\n",
       " 'any': 1,\n",
       " 'apartment': 2,\n",
       " 'apartment/NN': 1,\n",
       " 'aper': 1,\n",
       " 'are': 18,\n",
       " 'article': 2,\n",
       " 'as': 4,\n",
       " 'assign': 1,\n",
       " 'balls': 4,\n",
       " 'base': 3,\n",
       " 'based': 1,\n",
       " 'bases': 1,\n",
       " 'basic': 1,\n",
       " 'be': 9,\n",
       " 'been': 1,\n",
       " 'below': 1,\n",
       " 'biggest': 2,\n",
       " 'bing_key': 1,\n",
       " 'black': 2,\n",
       " 'black/JJ': 1,\n",
       " 'blue': 2,\n",
       " 'book': 1,\n",
       " 'books': 1,\n",
       " 'bought': 2,\n",
       " 'bought/VBN': 1,\n",
       " 'brazil': 1,\n",
       " 'break': 1,\n",
       " 'building': 2,\n",
       " 'buy': 1,\n",
       " 'by': 3,\n",
       " 'called': 1,\n",
       " 'can': 7,\n",
       " 'canonicalization': 1,\n",
       " 'car': 1,\n",
       " 'carry': 1,\n",
       " 'case': 1,\n",
       " 'cases': 1,\n",
       " 'categories': 1,\n",
       " 'chance': 1,\n",
       " 'china': 1,\n",
       " 'chop': 1,\n",
       " 'chosen': 1,\n",
       " 'chunker': 1,\n",
       " 'chunking': 2,\n",
       " 'cities': 1,\n",
       " 'city': 1,\n",
       " 'classify': 1,\n",
       " 'code': 6,\n",
       " 'collocation': 1,\n",
       " 'combinations': 1,\n",
       " 'common': 3,\n",
       " 'comparative': 1,\n",
       " 'complicated': 1,\n",
       " 'conclusion': 1,\n",
       " 'conference': 1,\n",
       " 'conference/NN': 1,\n",
       " 'connected': 1,\n",
       " 'considered': 1,\n",
       " 'considering': 1,\n",
       " 'constituent': 1,\n",
       " 'containing': 1,\n",
       " 'contains': 4,\n",
       " 'context': 1,\n",
       " 'converting': 2,\n",
       " 'coreference': 2,\n",
       " 'correct': 1,\n",
       " 'countries': 2,\n",
       " 'created': 1,\n",
       " 'data': 2,\n",
       " 'definition': 1,\n",
       " 'describe': 1,\n",
       " 'described': 6,\n",
       " 'detail': 1,\n",
       " 'determine': 1,\n",
       " 'diacritics': 1,\n",
       " 'discrete': 1,\n",
       " 'discuss': 1,\n",
       " 'discussed': 1,\n",
       " 'do': 2,\n",
       " 'does': 1,\n",
       " 'done': 3,\n",
       " 'draw': 2,\n",
       " 'duel': 1,\n",
       " 'e.g.': 2,\n",
       " 'each': 2,\n",
       " 'easily': 2,\n",
       " 'employment': 1,\n",
       " 'ending': 1,\n",
       " 'endings': 1,\n",
       " 'english': 1,\n",
       " 'entities': 2,\n",
       " 'entity': 2,\n",
       " 'etc': 3,\n",
       " 'example': 9,\n",
       " 'example\\\\t': 1,\n",
       " 'examples': 4,\n",
       " 'expanding': 1,\n",
       " 'expected': 1,\n",
       " 'expressions': 2,\n",
       " 'extract': 1,\n",
       " 'extraction': 8,\n",
       " 'extractor': 1,\n",
       " 'extractor.get_collocations_of_length': 1,\n",
       " 'find': 1,\n",
       " 'finds': 1,\n",
       " 'first': 1,\n",
       " 'following': 1,\n",
       " 'for': 20,\n",
       " 'for/IN': 3,\n",
       " 'form': 2,\n",
       " 'format': 1,\n",
       " 'forms': 2,\n",
       " 'found': 2,\n",
       " 'free': 2,\n",
       " 'from': 17,\n",
       " 'function': 1,\n",
       " 'further': 1,\n",
       " 'generation': 1,\n",
       " 'gensim': 1,\n",
       " 'get': 2,\n",
       " 'given': 2,\n",
       " 'grammatical': 1,\n",
       " 'group': 1,\n",
       " 'groups': 2,\n",
       " 'had': 1,\n",
       " 'have': 2,\n",
       " 'he': 4,\n",
       " 'he/PRP': 1,\n",
       " 'here': 2,\n",
       " 'higher': 1,\n",
       " 'human': 3,\n",
       " 'husband': 1,\n",
       " 'i': 3,\n",
       " 'identifies': 1,\n",
       " 'identifying': 1,\n",
       " 'if': 2,\n",
       " 'implementing': 1,\n",
       " 'import': 13,\n",
       " 'important': 1,\n",
       " 'in': 14,\n",
       " 'includes': 1,\n",
       " 'including': 2,\n",
       " 'india': 1,\n",
       " 'individuals': 1,\n",
       " 'indonesia': 1,\n",
       " 'inflectional': 1,\n",
       " 'inflections': 1,\n",
       " 'inflexional': 1,\n",
       " 'information': 2,\n",
       " 'input': 1,\n",
       " 'input=': 1,\n",
       " 'input_str': 19,\n",
       " 'input_str.lower': 1,\n",
       " 'input_str.strip': 1,\n",
       " 'input_str.translate': 1,\n",
       " 'input_str=': 4,\n",
       " 'input_str=word_tokenize': 2,\n",
       " 'interesting': 2,\n",
       " 'into': 3,\n",
       " 'is': 15,\n",
       " 'it': 3,\n",
       " 'its': 3,\n",
       " 'keep': 1,\n",
       " 'keys': 1,\n",
       " 'knowledge': 1,\n",
       " 'language': 8,\n",
       " 'languages': 1,\n",
       " 'leading': 3,\n",
       " 'lemmatization': 3,\n",
       " 'lemmatizer.lemmatize': 1,\n",
       " 'lemmatizer=WordNetLemmatizer': 1,\n",
       " 'length': 1,\n",
       " 'letters': 1,\n",
       " 'lexical': 1,\n",
       " 'libraries': 2,\n",
       " 'library': 1,\n",
       " 'like': 2,\n",
       " 'line': 2,\n",
       " 'links': 1,\n",
       " 'list': 1,\n",
       " 'locations': 2,\n",
       " 'look': 1,\n",
       " 'looked': 1,\n",
       " 'lower': 1,\n",
       " 'lowercase': 2,\n",
       " 'machine': 1,\n",
       " 'machine-readable': 1,\n",
       " 'main': 2,\n",
       " 'many': 1,\n",
       " 'marks': 2,\n",
       " 'married': 1,\n",
       " 'may': 1,\n",
       " 'meaning': 1,\n",
       " 'meanings': 1,\n",
       " 'mentions': 1,\n",
       " 'mice': 1,\n",
       " 'mind': 1,\n",
       " 'more': 3,\n",
       " 'morphological': 1,\n",
       " 'most': 1,\n",
       " 'mouse': 1,\n",
       " 'named': 2,\n",
       " 'named-entity': 1,\n",
       " 'namely': 1,\n",
       " 'names': 1,\n",
       " 'natural': 3,\n",
       " 'ne_chunk': 2,\n",
       " 'necessary': 1,\n",
       " 'needed': 1,\n",
       " 'new': 2,\n",
       " 'new/JJ': 1,\n",
       " 'nltk': 1,\n",
       " 'nltk.RegexpParser': 1,\n",
       " 'nltk.stem': 2,\n",
       " 'nltk.tokenize': 3,\n",
       " 'normalization': 4,\n",
       " 'not': 4,\n",
       " 'noun': 1,\n",
       " 'nouns': 2,\n",
       " 'numbers': 6,\n",
       " 'o56btained': 1,\n",
       " 'obtaining': 1,\n",
       " 'occurring': 1,\n",
       " 'of': 31,\n",
       " 'of/IN': 1,\n",
       " 'off': 1,\n",
       " 'often': 1,\n",
       " 'on': 5,\n",
       " 'open-source': 1,\n",
       " 'opposed': 1,\n",
       " 'or': 6,\n",
       " 'order': 1,\n",
       " 'organizations': 2,\n",
       " 'other': 2,\n",
       " 'others': 2,\n",
       " 'p': 1,\n",
       " 'parsing': 1,\n",
       " 'part': 2,\n",
       " 'particular': 3,\n",
       " 'parts': 2,\n",
       " 'people': 1,\n",
       " 'person': 1,\n",
       " 'persons': 1,\n",
       " 'phrases': 1,\n",
       " 'pieces': 1,\n",
       " 'platform': 2,\n",
       " 'population': 2,\n",
       " 'pos_check': 1,\n",
       " 'pos_tag': 2,\n",
       " 'possible': 3,\n",
       " 'post': 1,\n",
       " 'pre-defined': 1,\n",
       " 'preprocessing': 5,\n",
       " 'presented': 1,\n",
       " 'print': 11,\n",
       " 'process': 3,\n",
       " 'processing': 2,\n",
       " 'programs': 3,\n",
       " 'pronoun': 1,\n",
       " 'provides': 1,\n",
       " 'punctuation': 4,\n",
       " 'punctuations': 1,\n",
       " 'r': 1,\n",
       " 'raw': 1,\n",
       " 're': 1,\n",
       " 're.sub': 1,\n",
       " 'ready': 1,\n",
       " 'real-world': 1,\n",
       " 'recognition': 5,\n",
       " 'red': 4,\n",
       " 'reduce': 1,\n",
       " 'reducing': 1,\n",
       " 'refer': 1,\n",
       " 'referring': 1,\n",
       " 'refers': 1,\n",
       " 'reg_exp': 2,\n",
       " 'regular': 1,\n",
       " 'relations': 1,\n",
       " 'relationship': 2,\n",
       " 'relevant': 1,\n",
       " 'removal': 4,\n",
       " 'remove': 4,\n",
       " 'removed': 1,\n",
       " 'removes': 2,\n",
       " 'removing': 5,\n",
       " 'resolution': 8,\n",
       " 'result': 11,\n",
       " 'result.draw': 1,\n",
       " 'result.tags': 3,\n",
       " 'right': 1,\n",
       " 'root': 1,\n",
       " 'rp': 1,\n",
       " 'rp.parse': 1,\n",
       " 'rules': 1,\n",
       " 's': 5,\n",
       " 'said': 1,\n",
       " 'same': 2,\n",
       " 'scikit-learn': 1,\n",
       " 'second': 1,\n",
       " 'sentence': 3,\n",
       " 'sentences': 1,\n",
       " 'set': 2,\n",
       " 'sever': 1,\n",
       " 'several': 2,\n",
       " 'shallow': 1,\n",
       " 'sheet': 4,\n",
       " 'should': 1,\n",
       " 'simply': 1,\n",
       " 'sklearn.feature_extraction.stop_words': 1,\n",
       " 'smaller': 1,\n",
       " 'so': 2,\n",
       " 'so/IN': 1,\n",
       " 'some': 2,\n",
       " 'sources': 1,\n",
       " 'spaCy': 5,\n",
       " 'spaces': 3,\n",
       " 'spacy.lang.en.stop_words': 1,\n",
       " 'sparse': 3,\n",
       " 'speech': 6,\n",
       " 'splitting': 1,\n",
       " 'spouse': 1,\n",
       " 'start': 1,\n",
       " 'stated': 1,\n",
       " 'states': 1,\n",
       " 'statistical': 1,\n",
       " 'stem': 2,\n",
       " 'stemmer.stem': 1,\n",
       " 'stemmer=': 1,\n",
       " 'stemmers': 1,\n",
       " 'stemming': 7,\n",
       " 'step': 3,\n",
       " 'steps': 3,\n",
       " 'stop': 6,\n",
       " 'stop_words': 2,\n",
       " 'stopwords.words': 1,\n",
       " 'stove': 2,\n",
       " 'stove/NN': 1,\n",
       " 'string': 6,\n",
       " 'string.maketrans': 1,\n",
       " 'string.punctuation': 1,\n",
       " 'strip': 1,\n",
       " 'structure': 1,\n",
       " 'structured': 1,\n",
       " 'such': 2,\n",
       " 'suite': 1,\n",
       " 'symbolic': 1,\n",
       " 'symbols': 1,\n",
       " 'table': 5,\n",
       " 'taggers': 1,\n",
       " 'tagging': 4,\n",
       " 'talk': 1,\n",
       " 'talked': 1,\n",
       " 'task': 1,\n",
       " 'tasks': 1,\n",
       " 'techniques': 1,\n",
       " 'television': 2,\n",
       " 'television/NN': 1,\n",
       " 'terms': 3,\n",
       " 'text': 17,\n",
       " 'textblob': 2,\n",
       " 'texts': 2,\n",
       " 'than': 1,\n",
       " 'that': 5,\n",
       " 'the': 32,\n",
       " 'the/DT': 1,\n",
       " 'their': 1,\n",
       " 'them': 2,\n",
       " 'they': 1,\n",
       " 'this': 4,\n",
       " 'time': 1,\n",
       " 'times': 1,\n",
       " 'to': 28,\n",
       " 'to/TO': 1,\n",
       " 'together': 1,\n",
       " 'tokenization': 2,\n",
       " 'tokens': 4,\n",
       " 'tool': 1,\n",
       " 'tools': 12,\n",
       " 'transferring': 1,\n",
       " 'translation': 1,\n",
       " 'tree': 1,\n",
       " 'two': 1,\n",
       " 'type': 1,\n",
       " 'types': 1,\n",
       " 'u': 27,\n",
       " 'united': 1,\n",
       " 'units': 1,\n",
       " 'unstructured': 1,\n",
       " 'upper': 1,\n",
       " 'use': 2,\n",
       " 'used': 2,\n",
       " 'uses': 1,\n",
       " 'using': 11,\n",
       " 'usually': 1,\n",
       " 'verb': 1,\n",
       " 'verbs': 2,\n",
       " 'was': 1,\n",
       " 'we': 4,\n",
       " 'went': 1,\n",
       " 'went/VBD': 1,\n",
       " 'were': 2,\n",
       " 'were/VBD': 1,\n",
       " 'while': 2,\n",
       " 'white': 5,\n",
       " 'white/JJ': 1,\n",
       " 'whitespaces': 1,\n",
       " 'will': 3,\n",
       " 'with': 4,\n",
       " 'with.': 1,\n",
       " 'word': 8,\n",
       " 'word_tokenize': 6,\n",
       " 'words': 18,\n",
       " 'work': 2,\n",
       " 'works': 1,\n",
       " 'works/VBZ': 1,\n",
       " 'would': 2,\n",
       " 'write': 2,\n",
       " 'xrenner': 1,\n",
       " 'yesterday': 1,\n",
       " 'you': 1,\n",
       " 'your': 1,\n",
       " '{': 3,\n",
       " '|': 1,\n",
       " '}': 3,\n",
       " '~': 1,\n",
       " '—': 3,\n",
       " '‘': 41,\n",
       " '’': 104,\n",
       " '“': 30,\n",
       " '”': 41}"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document('file.txt').each_word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
